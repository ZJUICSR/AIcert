webpackJsonp([11],{GTM5:function(e,t,a){"use strict";var s=a("fZjL"),n=a.n(s),r=a("lHA8"),i=a.n(r),o=a("c/Tr"),d=a.n(o),c=a("XLwt"),u=a("nxKd"),l=a.n(u),m=a("r1Wa"),p=a.n(m),h={name:"autoAdvAttackEval",components:{},props:{isShow:{type:Boolean,default:!1,required:!0},result:{},postData:{}},data:function(){return{htmlTitle:"自动化对抗攻击评估报告",echart_init:!1,res:{},auto_method:{graph:"基于知识图谱的AI模型自动化攻防",graph_rule:"融合规则驱动的知识图谱自动化攻防",flow:"基于流水线的AI模型自动化攻防",rule:"规则驱动的AI模型自动化攻防",flow_rule:"融合规则驱动的流水线自动化攻防"}}},watch:{result:function(e,t){console.log("watch:",this.result),"Auto_Attack"in e&&this.updated()}},created:function(){console.log("created:",this.result),"Auto_Attack"in this.result&&this.updated()},methods:{closeMyself:function(){this.$emit("on-close")},_stopPropagation:function(e){e.stopPropagation()},defenseShow:function(){return(arguments.length>0&&void 0!==arguments[0]?arguments[0]:[]).join("、")},initGraph2:function(){console.log(l.a),console.log(p.a);var e=p.a.map(function(e){return e.target=e.dest,e}),t=l.a;console.log(l.a,e);var a=d()(new i.a(l.a.map(function(e){return e.method_type}))).map(function(e){return{name:e}}),s={color:"red",tooltip:{formatter:function(e){return console.log(e),"node"===e.dataType?"\n              <div>\n                <h1>\n                  攻击算法："+e.name+"\n                </h1>\n                <p>\n                  算法描述："+e.data.desc+"\n                </p>\n                <p >\n                  来源论文："+e.data.paper+"\n                </p>\n                <p>\n                <p>\n                  方法类型："+e.data.method_type+"\n                </p>\n              </div>\n            ":"\n              <div>\n                <h1>"+e.data.source+" > "+e.data.target+"</h1>\n                <h1>关系描述："+e.data.desc+"</h1>\n              </div>\n            "}},series:[{type:"graph",layout:"force",animation:!0,data:t.map(function(e){return e.category=a.find(function(t){return t.name===e.method_type}),e}),roam:!0,zoom:1.5,draggable:!0,edgeSymbol:["circle","arrow"],label:{show:!0,position:"right",formatter:"{b}"},emphasis:{focus:"adjacency",itemStyle:{color:"rgb(73,209,198)"}},force:{repulsion:100,edgeLength:5},edges:e}]};setTimeout(function(){var e=c.c(document.getElementById("graph_echart"));s&&e.setOption(s)},500)},initGraph3:function(e){var t=e,a={xAxis:{type:"category",data:n()(t),axisLabel:{color:"#000",interval:0},name:"算法名称",nameLocation:"center",nameGap:40,nameTextStyle:{fontSize:20,color:"black"}},tooltip:{trigger:"axis",axisPointer:{lineStyle:{color:"#000"}}},yAxis:{name:"准确率",nameLocation:"center",nameGap:40,nameTextStyle:{fontSize:20,color:"black"},axisLine:{lineStyle:{color:"#000"}},splitLine:{lineStyle:{color:"#57617B"}},type:"value",axisLabel:{formatter:"{value} %",fontSize:15,color:"#000",textBorderColor:"var(--gray-3, #6C7385);",textBorderWidth:.5,textBorderType:"solid"}},series:[{data:n()(t).map(function(e){return"ori"===e?{value:t[e],itemStyle:{color:"#a90000"}}:{value:t[e],itemStyle:{color:"rgba(11, 85, 244, 0.8)"}}}),type:"bar",barWidth:36}]};setTimeout(function(){var e=c.c(document.getElementById("test_echart"));a&&e.setOption(a)},500)},updated:function(){this.res={},this.res.label="",this.res.min_acc=100,this.res.Eva="";var e={};if("graph"==this.postData.AutoMethod){for(var t in this.initGraph2(),this.result.Auto_Attack[this.postData.AutoMethod])"recom_algorithm"!=t&&(e[t]=this.result.Auto_Attack[this.postData.AutoMethod][t].after_acc,e.ori=this.result.Auto_Attack[this.postData.AutoMethod][t].before_acc);e!={}&&(this.res.data=e,this.initGraph3(e))}else"graph_rule"==this.postData.AutoMethod?(this.initGraph2(),e=this.result.Auto_Attack[this.postData.AutoMethod].acc,this.initGraph3(e)):["flow","rule","flow_rule"].indexOf(this.postData.AutoMethod)>-1&&((e=this.result.Auto_Attack[this.postData.AutoMethod].attack_results).ori=this.result.Auto_Attack[this.postData.AutoMethod].ori_acc,this.initGraph3(e));for(var a in e)"ori"!=a&&this.res.min_acc>e[a]&&(this.res.min_acc=e[a],this.res.label=a);this.res.score=parseInt(this.res.min_acc),this.res.min_acc<30&&(this.res.score+=20),this.res.min_acc>=80?this.res.Eva="优秀":this.res.min_acc<80&&this.res.min_acc>=60?this.res.Eva="良好":this.res.Eva="差";var s=this.defenseShow(this.postData.AdvMethods);this.res.score_des=this.postData.ModelParam.name+"模型鲁棒性得分为"+this.res.score+"，是一个较"+this.res.Eva+"的模型，本次对抗攻击方法有"+s+"，其中"+this.res.label+"的攻击效果最佳"}}},b={render:function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("div",{staticClass:"dialog"},[e.isShow?s("div",{staticClass:"dialog-cover back",on:{click:e.closeMyself}}):e._e(),e._v(" "),s("transition",{attrs:{name:"drop"}},[e.isShow?s("div",{staticClass:"dialog-content",on:{click:function(t){return t.stopPropagation(),e._stopPropagation(t)}}},[s("div",{staticClass:"dialog_head back"},[s("div",{staticClass:"close_button"},[s("a-icon",{staticClass:"closebutton",staticStyle:{"font-size":"20px",color:"#6C7385"},attrs:{type:"close"},on:{click:e.closeMyself}})],1),e._v(" "),e._t("header",function(){return[s("div",{staticClass:"dialog-title"},[s("img",{staticStyle:{width:"50px",height:"50px"},attrs:{src:a("oh/m")}}),e._v("模型对抗性测试")])]})],2),e._v(" "),s("div",{staticClass:"dialog_main",attrs:{id:"pdfDom"}},[["graph","graph_rule"].indexOf(e.postData.AutoMethod)>-1?e._t("main",function(){return[Object.keys(e.postData).length>0?s("div",{staticStyle:{background:"var(--gray-7, #F2F4F9)",width:"100%",padding:"24px"}},[s("a-row",[s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("自动化攻防方案:")])]),e._v(" "),s("a-col",{attrs:{span:6}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.auto_method[e.postData.AutoMethod]))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("模型:")])]),e._v(" "),s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.ModelParam.name))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("数据集:")])]),e._v(" "),s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.DatasetParam.name))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("数据类型:")])]),e._v(" "),s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.data_type))])])],1),e._v(" "),s("a-row",[s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("攻击方式:")])]),e._v(" "),s("a-col",{attrs:{span:5}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.attack_mode))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("攻击类型:")])]),e._v(" "),s("a-col",{attrs:{span:5}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.attack_type))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("防御算法:")])]),e._v(" "),s("a-col",{attrs:{span:5}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.defend_algorithm))])])],1)],1):e._e(),e._v(" "),s("div",{staticClass:"graph_result"},[s("div",{staticClass:"bar_div"},[s("div",{staticClass:"result-title"},[e._v("机器学习攻防技战术知识图谱")]),e._v(" "),s("div",{staticClass:"graph_content"},[s("div",{attrs:{id:"graph_echart"}}),e._v(" "),s("div",{staticClass:"conclusion"},[s("p",{staticClass:"result_text"},[e._v("如上图所示是由各种攻击算法与防御算法构建的攻防技知识图谱，基于此可构建更科学的自动化测试方案。")])])])]),e._v(" "),s("div",{staticClass:"bar_div"},[s("div",{staticClass:"result-title"},[e._v("自动化测试方案及测试结果")]),e._v(" "),s("div",{staticClass:"graph_content"},[s("div",{attrs:{id:"test_echart"}}),e._v(" "),s("div",{staticClass:"conclusion"},[s("p",{staticClass:"result_text"},[e._v("本次测试制定的测试方案为"+e._s(e.defenseShow(e.result.Auto_Attack[e.postData.AutoMethod].recom_algorithm))+"，\n                    部分测试结果如上所示，其中"+e._s(e.res.label)+"攻击效果最佳，受攻击后的模型准确率为"+e._s(e.res.min_acc)+"。")])])])])]),e._v(" "),s("a-button",{staticStyle:{width:"160px",height:"40px","margin-bottom":"30px","margin-top":"10px","font-size":"18px",color:"white","background-color":"rgb(46, 56, 245)","border-radius":"8px"},on:{click:function(t){return e.getPdf()}}},[s("a-icon",{attrs:{type:"upload"}}),e._v("导出报告内容\n          ")],1)]}):e._e(),e._v(" "),["rule","flow_rule","flow"].indexOf(e.postData.AutoMethod)>-1?e._t("main",function(){return[Object.keys(e.postData).length>0?s("div",{staticStyle:{background:"var(--gray-7, #F2F4F9)",width:"100%",padding:"24px"}},[s("a-row",[s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("自动化攻防方案:")])]),e._v(" "),s("a-col",{attrs:{span:6}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.auto_method[e.postData.AutoMethod]))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("模型:")])]),e._v(" "),s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.ModelParam.name))])]),e._v(" "),s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("数据集:")])]),e._v(" "),s("a-col",{attrs:{span:3}},[s("div",{staticClass:"grid-content-value"},[e._v(e._s(e.postData.DatasetParam.name))])])],1),e._v(" "),s("a-row",[s("a-col",{attrs:{span:2}},[s("div",{staticClass:"grid-content-name",staticStyle:{color:"#6C7385"}},[e._v("攻击算法:")])]),e._v(" "),s("a-col",{attrs:{span:22}},[s("div",{staticClass:"grid-content-value",staticStyle:{"text-align":"left"}},[e._v(e._s(e.defenseShow(e.postData.AdvMethods)))])])],1)],1):e._e(),e._v(" "),s("div",{staticClass:"result-title"},[e._v("模型鲁棒性得分")]),e._v(" "),s("div",{staticStyle:{position:"relative","text-align":"center"}},[s("div",{staticStyle:{width:"100%",height:"300px",position:"absolute","text-align":"center","line-height":"230px","font-size":"80px",color:"white","font-weight":"bold"}},[e._v(" "+e._s(e.res.score)+" ")]),e._v(" "),s("div",{staticStyle:{width:"100%",height:"300px",position:"absolute","text-align":"center","line-height":"330px","font-size":"20px",color:"white"}},[e._v(" "+e._s(e.res.Eva)+"  ")]),e._v(" "),s("img",{staticStyle:{width:"300px",height:"300px"},attrs:{src:a("cMrJ")}}),e._v(" "),s("div",{staticClass:"conclusion"},[s("p",{staticClass:"result_text"},[e._v(e._s(e.res.score_des))])])]),e._v(" "),s("div",{staticClass:"bar_div"},[s("div",{staticClass:"result-title"},[e._v("对抗性测试方案生成及自动化对抗测试评估")]),e._v(" "),s("div",{staticClass:"graph_content"},[s("div",{attrs:{id:"test_echart"}}),e._v(" "),s("div",{staticClass:"conclusion"},[s("p",{staticClass:"result_text"},[e._v("本次测试制定的测试方案为"+e._s(e.defenseShow(e.postData.AdvMethods))+"，\n                  部分测试结果如上所示，其中"+e._s(e.res.label)+"攻击效果最佳，受攻击后的模型准确率为"+e._s(e.res.min_acc)+"。")])])])]),e._v(" "),s("a-button",{staticStyle:{width:"160px",height:"40px","margin-bottom":"30px","margin-top":"10px","font-size":"18px",color:"white","background-color":"rgb(46, 56, 245)","border-radius":"8px"},on:{click:function(t){return e.getPdf()}}},[s("a-icon",{attrs:{type:"upload"}}),e._v("导出报告内容\n          ")],1)]}):e._e()],2)]):e._e()])],1)},staticRenderFns:[]};var v=a("VU/8")(h,b,!1,function(e){a("VuTu")},"data-v-26b9bffc",null);t.a=v.exports},JdmF:function(e,t,a){e.exports=a.p+"static/img/NashBalanceImg.d23368c.png"},RzuM:function(e,t){},VuTu:function(e,t){},XNgg:function(e,t,a){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var s=a("gRE1"),n=a.n(s),r=a("mvHQ"),i=a.n(r),o=a("R45V"),d=a("83tA"),c=a("T9rv"),u=a("UI/F"),l=a("CSsb"),m=a.n(l),p=a("2b9l"),h=a.n(p),b=a("7wpf"),v=a("hcOA"),f=a("0zDd"),y=a("GTM5"),g=a("f4au"),_={template:'\n       <svg t="1680138013828" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4354" width="128" height="128"><path d="M534.869333 490.496a1403.306667 1403.306667 0 0 0 50.858667-25.813333c16.042667-8.618667 29.013333-15.061333 38.570667-19.029334 9.557333-3.925333 17.066667-6.058667 22.869333-6.058666 9.557333 0 17.749333 3.2 24.917333 10.026666 6.826667 6.826667 10.581333 15.061333 10.581334 25.088 0 5.76-1.706667 11.818667-5.12 17.92-3.413333 6.101333-7.168 10.069333-10.922667 11.861334-35.157333 14.677333-74.410667 25.429333-116.736 31.872 7.850667 7.168 17.066667 17.237333 28.330667 29.781333 11.264 12.544 17.066667 18.986667 17.749333 20.053333 4.096 6.101333 9.898667 13.653333 17.408 22.613334 7.509333 8.96 12.629333 15.786667 15.36 20.778666 2.730667 5.034667 4.437333 11.093333 4.437333 18.304a33.706667 33.706667 0 0 1-9.898666 24.021334 33.834667 33.834667 0 0 1-25.6 10.410666c-10.24 0-22.186667-8.618667-35.157334-25.472-12.970667-16.512-30.037333-46.933333-50.517333-91.050666-20.821333 39.424-34.816 65.962667-41.642667 78.506666-7.168 12.544-13.994667 22.186667-20.48 28.672a30.976 30.976 0 0 1-22.528 9.685334 32.256 32.256 0 0 1-25.258666-11.093334 35.413333 35.413333 0 0 1-9.898667-23.68c0-7.893333 1.365333-13.653333 4.096-17.578666 25.258667-35.84 51.541333-67.413333 78.848-93.568a756.650667 756.650667 0 0 1-61.44-12.544 383.061333 383.061333 0 0 1-57.685333-20.48c-3.413333-1.749333-6.485333-5.717333-9.557334-11.818667a30.208 30.208 0 0 1-5.12-16.853333 32.426667 32.426667 0 0 1 10.581334-25.088 33.152 33.152 0 0 1 24.234666-10.026667c6.485333 0 14.677333 2.133333 24.576 6.101333 9.898667 4.266667 22.186667 10.026667 37.546667 18.261334 15.36 7.893333 32.426667 16.853333 51.882667 26.538666-3.413333-18.261333-6.485333-39.082667-8.874667-62.378666-2.389333-23.296-3.413333-39.424-3.413333-48.042667 0-10.752 3.072-19.712 9.557333-27.264A30.677333 30.677333 0 0 1 512.341333 341.333333c9.898667 0 18.090667 3.925333 24.576 11.477334 6.485333 7.893333 9.557333 17.92 9.557334 30.464 0 3.584-0.682667 10.410667-1.365334 20.48-0.682667 10.368-2.389333 22.570667-4.096 36.906666-2.048 14.677333-4.096 31.146667-6.144 49.834667z" fill="#FF3838" p-id="4355"></path></svg>\n       '},A={template:'\n           <a-icon :component="selectSvg" />\n       ',data:function(){return{selectSvg:_}}},k={name:"autoAttack",components:{navmodule:o.a,func_introduce:d.a,showLog:c.a,resultDialog:u.a,selectIcon:A,DataSetCard:v.a,ModelCard:f.a,DefenseCard:g.a,autoAdvAttackEval:y.a,MethodCard:b.a},data:function(){return{methodHoverIndex:-1,methodDescription:"",htmlTitle:"模型对抗性测试报告",radioStyle:{display:"block",lineHeight:"30px",width:"100%","margin-bottom":"16px"},dataSetInfo:[{name:"CIFAR10",class:["airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck"],description:"是由 Hinton 的学生 Alex Krizhevsky 和 Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含 10 个类别的 RGB 彩色图 片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。图片的尺寸为 32×32 ，数据集中一共有 50000 张训练圄片和 10000 张测试图片。",pictureSrcs:[[a("HiaR"),a("DJt5"),a("S99w"),a("J598"),a("/pRs"),a("YuLH"),a("Nvyw"),a("lB35"),a("dKp5"),a("NgyD")]]},{name:"MNIST",description:"是一个手写体数字的图片数据集，该数据集来由美国国家标准与技术研究所（National Institute of Standards and Technology (NIST)）发起整理，一共统计了来自250个不同的人手写数字图片，其中50%是高中生，50%来自人口普查局的工作人员。该数据集的收集目的是希望通过算法，实现对手写数字的识别。",class:["数字0","数字1","数字2","数字3","数字4","数字5","数字6","数字7","数字8","数字9"],pictureSrcs:[[a("wlDy"),a("KdR5"),a("2LXz"),a("3rtV"),a("Ppgw"),a("P9ea"),a("ec3M"),a("wAAx"),a("30PV"),a("v3v+")]]}],selectedDataset:0,modelInfo:[{name:"ResNet18",layer:18},{name:"ResNet34",layer:34},{name:"ResNet50",layer:50},{name:"ResNet101",layer:101},{name:"ResNet152",layer:152}],selectedModel:0,attackMode:"white_box",attackType:"evasion_attack",defendAlgorithm:"Adversarial-Training",dataType:"image",matchedMethod:"graph",defenseList:["Adversarial-Training","IBP-CROWN","Defense-GAN","InvGAN","SID","ADC-detector","DIP","SmsNet","Activation Clustering","Neural Cleanse","Deep k-NN","ATMBPR","SAO","DAVE","APT","FNCF","AMR","BAT","TextFirewall","RanMask","FGWS","Safer","AMDA","DISP","Robust-Word-Recognition"],methodInfo:[{name:"FGSM",id:"FGSM",description:"Fast Gradient Sign MethodFGSM快速梯度符号法是一种简单而有效的生成对抗样本的方法，其工作方式如下：在给定输入数据后，利用已训练的模型输出预测并计算损失函数的梯度，然后使用梯度的符号来创建使损失最大化的新数据",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"范数",key:"norm",defaultNumber:0,number:0,type:"selectgroup",valuelist:["1","2","inf"]}]]},{name:"BIM",id:"BIM",description:"Basic Iterative MethodBIM迭代式FGSM是对FGSM的改进方法，主要的改进有两点，其一是FGSM方法是一步完成的，而BIM方法通过多次迭代来寻找对抗样本；其次，为了避免迭代过程中出现超出有效值的情况出现，使用了一个修建方法严格限制像素值的范围",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"步长参数",key:"eps_step",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:1,step:1},{name:"范数",key:"norm",defaultNumber:2,number:2,type:"selectgroup",valuelist:[1,2,"inf"]}]]},{name:"PGDL1",id:"PGDL1",description:"Projected Gradient DescentPGD投影梯度下降法是FGSM的迭代版本，该方法思路和BIM基本相同，不同之处在于该方法在迭代过程中使用范数投影的方法来约束非法数据，并且相对于BIM有一个随机的开始噪声。支持L1 norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.3,number:0,type:"inputNumber",min:0,step:.1},{name:"步长参数",key:"eps_step",defaultNumber:.1,number:0,type:"inputNumber",min:0,step:.1},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:1,step:1},{name:"随机开始次数",key:"num_random_init",defaultNumber:1,number:0,type:"inputNumber",min:0,step:1}]]},{name:"PGDL2",id:"PGDL2",description:"Projected Gradient DescentPGD投影梯度下降法是FGSM的迭代版本，该方法思路和BIM基本相同，不同之处在于该方法在迭代过程中使用范数投影的方法来约束非法数据，并且相对于BIM有一个随机的开始噪声。支持L2 norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.3,number:0,type:"inputNumber",min:0,step:.1},{name:"步长参数",key:"eps_step",defaultNumber:.1,number:0,type:"inputNumber",min:0,step:.1},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:1,step:1},{name:"随机开始次数",key:"num_random_init",defaultNumber:1,number:0,type:"inputNumber",min:0,step:1}]]},{name:"PGDLinf",id:"PGDLinf",description:"Projected Gradient DescentPGD投影梯度下降法是FGSM的迭代版本，该方法思路和BIM基本相同，不同之处在于该方法在迭代过程中使用范数投影的方法来约束非法数据，并且相对于BIM有一个随机的开始噪声。支持Linf norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.3,number:0,type:"inputNumber",min:0,step:.1},{name:"步长参数",key:"eps_step",defaultNumber:.1,number:0,type:"inputNumber",min:0,step:.1},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:1,step:1},{name:"随机开始次数",key:"num_random_init",defaultNumber:1,number:0,type:"inputNumber",min:0,step:1}]]},{name:"C&W",id:"C&W",description:"该方法的出发点是攻击比较有名的对抗样本防御方法-防御蒸馏(就防御蒸馏方法而言，它在基本的L-BFGS，FGSM攻击方法上表现本身就比较差)。对于寻找对抗样本过程中目标函数的设置将会极大的影响对抗样本的攻击效果，为此，通过目标函数的设定，在零范数，二范数和无穷范数的限制下分别设计了三种不同的寻找对抗样本的目标函数，这三种方法均可以绕过防御蒸馏的防御",attributes:[[{name:"最大迭代次数",key:"max_iterations",defaultNumber:1e3,number:0,type:"inputNumber",min:1,step:1},{name:"优化器学习率",key:"lr",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01}]]},{name:"DeepFool",id:"DeepFool",description:"DeepFool方法的出发点是想要精确的度量模型对于对抗样本的鲁棒性，为此提出了鲁棒性定义和计算方法。最终使用该计算方法生成对抗样本",attributes:[[{name:"最大迭代次数",key:"max_iter",defaultNumber:50,number:0,type:"inputNumber",min:1,step:1},{name:"扰动系数",key:"eta",defaultNumber:.02,number:0,type:"inputNumber",min:0,step:.01}]]},{name:"JSMA",id:"JacobianSaliencyMap",description:"该方法通过输入样本每一个位置的前向导数，借鉴模型解释性方法中的显著图理论，构造了一种对抗显著图。对抗显著图上的点的值就是样本中每一个点对每个目标类的前向导数大小，直觉上来讲，其反映的是该样本点对生成某一个目标类对抗样本的贡献度。那么使用该对抗显著图，确定需要攻击到的目标类后，便可以从该图中选择对目标类对抗影响最大的点进行修改即可。",attributes:[[{name:"噪声大小",key:"theta",defaultNumber:.1,number:0,type:"inputNumber",step:.1},{name:"允许修改的最大特征百分比",key:"gamma",defaultNumber:1,number:0,type:"inputNumber",min:0,max:1,step:.1}]]},{name:"Brendel&BethgeAttack",id:"Brendel&BethgeAttack",description:"边界攻击的基本思想是从一个已经是对抗样本的点(无目标攻击随意选择，有目标攻击选择目标类图片)开始，通过随机游走算法来减少对抗样本的强度，使其在决策边界上和被攻击的样本接近",attributes:[[{name:"正交步骤的初始步长",key:"delta",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01},{name:"趋近步骤的初始步长大小",key:"eps",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01},{name:"调整步长因数",key:"step_adapt",defaultNumber:.667,number:0,type:"inputNumber",min:0,max:1,step:.001},{name:"最大迭代次数",key:"max_iter",defaultNumber:1e3,number:0,type:"inputNumber",min:1,step:1}],[{name:"每次迭代的最大判定数",key:"num_trial",defaultNumber:25,number:0,type:"inputNumber",min:1,step:1},{name:"每次判定的样本数目",key:"sample_size",defaultNumber:20,number:0,type:"inputNumber",min:1,step:1},{name:"初始化的判定次数",key:"init_size",defaultNumber:100,number:0,type:"inputNumber",min:1,step:1}]]},{name:"UniversalPerturbationL1",id:"UniversalPerturbationL1",description:"在一个给定模型和数据集上，通过迭代算法寻找一个让大多数图片都被错误分类的对抗噪声。支持L1 norm范数",attributes:[[{name:"攻击方法",key:"attacker",defaultNumber:0,number:0,type:"selectgroup",valuelist:["FGSM","BIM","PGD","DeepFool","JSMA","Carlini_l2","Carlini_inf","Simba"]}],[{name:"扰动系数",key:"eps",defaultNumber:.078,number:0,type:"inputNumber",min:0,step:.001},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:0,step:1}]]},{name:"UniversalPerturbationL2",id:"UniversalPerturbationL2",description:"在一个给定模型和数据集上，通过迭代算法寻找一个让大多数图片都被错误分类的对抗噪声。支持L2 norm范数",attributes:[[{name:"攻击方法",key:"attacker",defaultNumber:0,number:0,type:"selectgroup",valuelist:["FGSM","BIM","PGD","DeepFool","JSMA","Carlini_l2","Carlini_inf","Simba"]}],[{name:"扰动系数",key:"eps",defaultNumber:.078,number:0,type:"inputNumber",min:0,step:.001},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:0,step:1}]]},{name:"UniversalPerturbationLinf",id:"UniversalPerturbationLinf",description:"在一个给定模型和数据集上，通过迭代算法寻找一个让大多数图片都被错误分类的对抗噪声。支持Linf norm范数",attributes:[[{name:"攻击方法",key:"attacker",defaultNumber:0,number:0,type:"selectgroup",valuelist:["FGSM","BIM","PGD","DeepFool","JSMA","Carlini_l2","Carlini_inf","Simba"]}],[{name:"扰动系数",key:"eps",defaultNumber:.078,number:0,type:"inputNumber",min:0,step:.001},{name:"最大迭代次数",key:"max_iter",defaultNumber:20,number:0,type:"inputNumber",min:0,step:1}]]},{name:"AutoAttackL1",id:"AutoAttackL1",description:"该方法首先改进了PGD方法的步长调整方法以及其的损失函数，并且进一步将白盒的FAB攻击和黑盒的Square攻击集成到了框架中。支持L1 norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"步长参数",key:"eps_step",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01}]]},{name:"AutoAttackL2",id:"AutoAttackL2",description:"该方法首先改进了PGD方法的步长调整方法以及其的损失函数，并且进一步将白盒的FAB攻击和黑盒的Square攻击集成到了框架中。支持L2 norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"步长参数",key:"eps_step",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01}]]},{name:"AutoAttackLinf",id:"AutoAttackLinf",description:"该方法首先改进了PGD方法的步长调整方法以及其的损失函数，并且进一步将白盒的FAB攻击和黑盒的Square攻击集成到了框架中。支持Linf norm范数",attributes:[[{name:"扰动系数",key:"eps",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"步长参数",key:"eps_step",defaultNumber:.01,number:0,type:"inputNumber",min:0,step:.01}]]},{name:"GD-UAP",id:"GD-UAP",description:"该方法在UAP工作的基础上提出了数据不相关的通用对抗噪声。与UAP生成的基本方法不同，该方法以对一个神经网络所有神经元产生更大的输出为目标，即以寻求那些可以使网络整体神经元更加激活的输入噪声为目标，并且将该噪声作为通用对抗噪声",attributes:[[{name:"最小饱和度",key:"sat_min",defaultNumber:.35,number:0,type:"inputNumber",min:0,max:1,step:.01},{name:"饱和度阈值",key:"sat_threshold",defaultNumber:1e-5,number:0,type:"inputNumber",min:0,max:1,step:1e-5},{name:"扰动系数",key:"eps",defaultNumber:.078,number:0,type:"inputNumber",min:0,step:.001}],[{name:"最大迭代次数",key:"max_iter",defaultNumber:1e4,number:0,type:"inputNumber",min:1,step:1},{name:"耐心间隔",key:"patience_interval",defaultNumber:10,number:0,type:"inputNumber",min:0,step:1}]]},{name:"SquareAttackL1",id:"SquareAttackL1",description:"该方法的主体思路是使用优化理论中的随机搜索方法来寻找对抗噪声，这种随机搜索的方法很多黑盒攻击方法都采用过。随机搜索的总体框架大体相同，如下图所示。基本思路是在在一个采用空间中随机选择一个噪声，如果该噪声能够降低目标函数的损失则将其添加到原图片上，否则进入下一轮的随机搜索。支持L1 norm范数。",attributes:[[{name:"初始化百分比",key:"p_init",defaultNumber:.05,number:0,type:"inputNumber",min:0,step:.01},{name:"重启次数",key:"n_restarts",defaultNumber:1,number:0,type:"inputNumber",min:1,step:1},{name:"loss类型",key:"loss_type",defaultNumber:0,number:0,type:"selectgroup",valuelist:["margin","ce"]}]]},{name:"SquareAttackL2",id:"SquareAttackL2",description:"该方法的主体思路是使用优化理论中的随机搜索方法来寻找对抗噪声，这种随机搜索的方法很多黑盒攻击方法都采用过。随机搜索的总体框架大体相同，如下图所示。基本思路是在在一个采用空间中随机选择一个噪声，如果该噪声能够降低目标函数的损失则将其添加到原图片上，否则进入下一轮的随机搜索。支持L2 norm范数。",attributes:[[{name:"初始化百分比",key:"p_init",defaultNumber:.05,number:0,type:"inputNumber",min:0,step:.01},{name:"重启次数",key:"n_restarts",defaultNumber:1,number:0,type:"inputNumber",min:1,step:1},{name:"loss类型",key:"loss_type",defaultNumber:0,number:0,type:"selectgroup",valuelist:["margin","ce"]}]]},{name:"SquareAttackLinf",id:"SquareAttackLinf",description:"该方法的主体思路是使用优化理论中的随机搜索方法来寻找对抗噪声，这种随机搜索的方法很多黑盒攻击方法都采用过。随机搜索的总体框架大体相同，如下图所示。基本思路是在在一个采用空间中随机选择一个噪声，如果该噪声能够降低目标函数的损失则将其添加到原图片上，否则进入下一轮的随机搜索。支持Linf norm范数。",attributes:[[{name:"初始化百分比",key:"p_init",defaultNumber:.05,number:0,type:"inputNumber",min:0,step:.01},{name:"重启次数",key:"n_restarts",defaultNumber:1,number:0,type:"inputNumber",min:1,step:1},{name:"loss类型",key:"loss_type",defaultNumber:0,number:0,type:"selectgroup",valuelist:["margin","ce"]}]]},{name:"HSJA",id:"HSJA",description:"在优化框架下的基于决策的攻击，并提出了一系列新颖的算法，用于生成针对性和非针对性的对抗性示例，这些示例针对“ 2-距离”或“∞距离”的最小距离进行了优化。 该算法本质上是迭代的，每个迭代涉及三个步骤：梯度方向的估计，通过几何级数进行的步长搜索和通过二分法的边界搜索。对优化框架和梯度方向估计进行了理论分析。这不仅为选择超参数提供了参考，而且还激发了所提出算法中的必要步骤",attributes:[[{name:"梯度估计的最大次数",key:"max_eval",defaultNumber:1e3,number:0,type:"inputNumber",min:1,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:50,number:0,type:"inputNumber",min:1,step:1},{name:"范数",key:"norm",defaultNumber:0,number:0,type:"selectgroup",valuelist:[2,"inf"]}],[{name:"梯度估计的初始次数",key:"init_eval",defaultNumber:100,number:0,type:"inputNumber",min:1,step:1},{name:"初始最大试验次数",key:"init_size",defaultNumber:100,number:0,type:"inputNumber",min:1,step:1}]]},{name:"PixelAttack",id:"PixelAttack",description:"使用差分进化算法寻找满足零范数约束的对抗样本",attributes:[[{name:"允许修改的像素数目",key:"th",defaultNumber:1,number:0,type:"inputNumber",min:1,max:784,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:5,number:0,type:"inputNumber",min:1,step:1},{name:"进化算法",key:"es",defaultNumber:0,number:0,type:"selectgroup",valuelist:["DE","CMAES"]}]]},{name:"SimBA",id:"SimBA",description:"该方法的基本思想为对于图片空间的任意一个方向，添加或者减少一个扰动值，总是有可能让神经网络对于该图片属于某个类别的分数变得更低或者更高，因此论文通过随机选择一些坐标点，通过添加或者减少扰动值，使图片分类到某个类别的分数总是往更低的方向去，从而最终导致图片分类错误。这可以看作是一种贪婪的选择策略，每一步都选择对于自己最优的方向，以此来接近全局最优",attributes:[[{name:"过冲参数",key:"epsilon",defaultNumber:.031,number:0,type:"inputNumber",min:0,step:.001},{name:"最大迭代次数",key:"max_iter",defaultNumber:3e3,number:0,type:"inputNumber",min:0,step:1},{name:"攻击方法",key:"attack",defaultNumber:0,number:0,type:"selectgroup",valuelist:["dct","px"]},{name:"pixel攻击的顺序",key:"order",defaultNumber:0,number:0,type:"selectgroup",valuelist:["random","diag"]}]]},{name:"ZOO",id:"ZOO",description:"在一个给定模型和数据集上，通过迭代算法寻找一个让大多数图片都被错误分类的对抗噪声。支持Linf norm范数",attributes:[[{name:"步长",key:"step_size",defaultNumber:.01,number:0,type:"inputNumber",min:0,max:1,step:.01},{name:"最大迭代次数",key:"max_iter",defaultNumber:3e3,number:0,type:"inputNumber",min:1,step:1},{name:"初始常量",key:"initial_const",defaultNumber:.01,number:0,type:"inputNumber",min:0,max:1,step:.01},{name:"置信度",key:"confidence",defaultNumber:0,number:0,type:"inputNumber",min:0,step:.001}]]},{name:"GeoDAL1",id:"GeoDAL1",description:"该方法基于神经网络决策边界往往具有较低曲率的事实假设，提出了一种在线性框架下通过法向量估计的方法完成黑盒基于决策模式下的对抗样本搜寻的方法。支持L1 norm范数",attributes:[[{name:"二维频率空间维数",key:"sub_dim",defaultNumber:10,number:0,type:"inputNumber",min:1,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:1,number:0,type:"inputNumber",min:1,step:1},{name:"高斯扰动方差",key:"sigma",defaultNumber:2e-4,number:0,type:"inputNumber",min:0,step:1e-4},{name:"λ参数",key:"lambda_param",defaultNumber:.6,number:0,type:"inputNumber",min:0,step:.1}]]},{name:"GeoDAL2",id:"GeoDAL2",description:"该方法基于神经网络决策边界往往具有较低曲率的事实假设，提出了一种在线性框架下通过法向量估计的方法完成黑盒基于决策模式下的对抗样本搜寻的方法。支持L2 norm范数",attributes:[[{name:"二维频率空间维数",key:"sub_dim",defaultNumber:10,number:0,type:"inputNumber",min:1,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:1,number:0,type:"inputNumber",min:1,step:1},{name:"高斯扰动方差",key:"sigma",defaultNumber:2e-4,number:0,type:"inputNumber",min:0,step:1e-4},{name:"λ参数",key:"lambda_param",defaultNumber:.6,number:0,type:"inputNumber",min:0,step:.1}]]},{name:"GeoDALinf",id:"GeoDALinf",description:"该方法基于神经网络决策边界往往具有较低曲率的事实假设，提出了一种在线性框架下通过法向量估计的方法完成黑盒基于决策模式下的对抗样本搜寻的方法。支持Linf norm范数",attributes:[[{name:"二维频率空间维数",key:"sub_dim",defaultNumber:10,number:0,type:"inputNumber",min:1,step:1},{name:"最大迭代次数",key:"max_iter",defaultNumber:1e3,number:0,type:"inputNumber",min:1,step:1},{name:"高斯扰动方差",key:"sigma",defaultNumber:2e-4,number:0,type:"inputNumber",min:0,step:1e-4},{name:"λ参数",key:"lambda_param",defaultNumber:.6,number:0,type:"inputNumber",min:0,step:.1}]]},{name:"Fastdrop",id:"Fastdrop",description:"该方法利用图像频域修改更加容易产生高质量对抗样本的思想，通过将图像变换到频域后，使用简单的频域小量随机丢弃和还原的搜索策略完成攻击",attributes:[]}],methodInfoNoParam:[[{name:"DIFGSM",description:"DIFGSM算法：Diverse Inputs Iterative Fast Gradient Sign Method,通过创建多样的输入模式提高对抗样本的迁移性。做法是对输入的原图像以p的概率加上随机且可导的变换(transformation)，使用梯度的方法最大化模型对变换后的原图像的损失函数值从而得到对抗图像"},{name:"FAB",description:"FAB算法：Fast Adaptive Boundary Attack使用类似DeepFool投影到决策平面的方法，主要不同的是，FAB将过程中的扰动尽可能偏向原样本点，从而可以找到能改变分类器输出的最小扰动。扰动的衡量距离为 linf"},{name:"FABL1",description:"FABL1算法：Fast Adaptive Boundary Attack使用类似DeepFool投影到决策平面的方法，主要不同的是，FAB将过程中的扰动尽可能偏向原样本点，从而可以找到能改变分类器输出的最小扰动。扰动的衡量距离为 l1"},{name:"FABL2",description:"FABL2算法：Fast Adaptive Boundary Attack使用类似DeepFool投影到决策平面的方法，主要不同的是，FAB将过程中的扰动尽可能偏向原样本点，从而可以找到能改变分类器输出的最小扰动。扰动的衡量距离为 l2"},{name:"EOTPGD",description:"EOTPGD算法：结合Expectation Over Transformation算法生成具有鲁棒性的对抗样本，能在变换(transformation)后依旧保持对抗性"}],[{name:"FFGSM",description:"FFGSM算法：在使用FGSM攻击算法前加入随机初始化的扰动，经过实验发现基于FFGSM的对抗训练拥有高效性"},{name:"Jitter",description:"Jitter算法：使用新的损失函数从而改善效率和攻击成功率，同时通过输出logits标准化到固定值范围将规模不变性引入损失函数"},{name:"MIFGSM",description:"MIFGSM算法：momentum iterative FGSM是一种使用momentum迭代梯度的方法，该方法在迭代梯度对抗攻击(如BIM)的基础上，累计每次梯度方向的速度向量作为momentum，每次对抗扰动不再直接使用梯度方向，转而采用momentum方向，从而稳定更新方向并避免局部极值，更好提高攻击迁移性"},{name:"NIFGSM",description:"NIFGSM算法：通过使用Nesterov Accelerated Gradient(NAG)到迭代梯度攻击方法中，由于NAG对之前累积的梯度进行修正，有助于有效地预测未来，从而使攻击更具有鲁棒性"},{name:"PGDRS",description:"PGDRS算法：randmized smoothing是提高模型鲁棒性的一种高效方法，PGD for randmized smoothing则是争对这类randmized smoothing模型的pgd对抗攻击方法。攻击norm为linf"}],[{name:"PGDRSL2",description:"PGDRSL2算法：randmized smoothing是提高模型鲁棒性的一种高效方法，PGD for randmized smoothing则是争对这类randmized smoothing模型的pgd对抗攻击方法。攻击norm为l2"},{name:"RFGSM",description:"RFGSM算法：R+FGSM在FGSM中加入随机的步骤, 是一个在白盒设置下高效的能替代迭代攻击的方法"},{name:"SINIFGSM",description:"SINIFGSM算法：SCALE-INVARIANT NIFGSM通过使用模型放大的方法实现多模型的组合攻击，使得攻击避免在白盒设置下“过拟合”，从而生成更具有迁移性的对抗样本"},{name:"SparseFool",description:"SparseFool算法：利用决策边界的低均值曲率计算对抗稀疏(sparse)扰动"},{name:"SPSA",description:"SPSA算法：SPSA adversarial attack是一种黑盒攻击方法，使用SPSA方法近似loss对样本的梯度，利用迭代梯度更新方法生成对抗样本"}],[{name:"TIFGSM",description:"TIFGSM算法：Translation-Invarient FGSM将translation-invarient attack与FGSM结合，具体是将对抗目标调整为使像素平移后的对抗样本输出与真实标签的loss最大，同时使用卷积梯度的方法减小计算代价，使用该攻击方法拥有更强的迁移性"},{name:"TPGD",description:"TPGD算法：基于KL-Divergence loss的pgd攻击"},{name:"VMIFGSM",description:"VMIFGSM算法：variance tuning MIFGSM在MIFGSM方法上，不再直接使用当前计算的梯度做momentum累加，而是基于先前迭代下的梯度方差调整当前梯度"},{name:"VNIFGSM",description:"VNIFGSM算法：variance tuning MIFGSM在NIFGSM方法上，不再直接使用当前计算的梯度做momentum累加，而是基于先前迭代下的梯度方差调整当前梯度"}],[{name:"AutoPGDL1",description:"一种基于L1范数正则化的对抗性攻击算法，通过迭代生成对抗样本，以欺骗机器学习模型并导致错误的预测结果。该算法利用L1范数正则化来控制对抗样本的扰动大小，使得扰动更加稀疏"},{name:"AutoPGDL2",description:"一种基于L2范数正则化的对抗攻击算法，通过迭代生成对抗样本，以欺骗机器学习模型。一定程度上成功地改变了原始样本的特征，导致模型产生错误的预测结果"},{name:"AutoPGDLinf",description:"基于1-无穷范数正则化的AutoPGD方法的对抗性攻击算法，在AutoPGDLinf算法中，对抗样本的生成过程是通过最小化原始样本与对抗样本之间的距离，并同时最大化对抗样本的损失函数来实现的。为了限制对抗样本的扰动范围，算法使用了1-无穷范数正则化，即对扰动进行约束，使其在每个维度上的取值范围不超过一个预先设定的阈值"},{name:"Auto-CGL1",description:"基于L1范数正则化的Auto-CGD方法的对抗性攻击算法。该算法通过最小化目标函数来寻找最优的扰动，使得扰动后的输入数据能够误导模型产生错误的预测结果"}],[{name:"Auto-CGL2",description:"基于L2范数正则化的Auto-CGD方法的对抗性攻击算法，通过迭代优化的方式生成对抗样本。它首先选择一个初始的对抗样本，然后通过迭代的方式不断调整该样本，使其能够最大程度地欺骗目标模型"},{name:"Auto-CGLinf",description:"基于1-无穷范数正则化的Auto-CGD方法的对抗性攻击算法。通过计算原始样本与对抗样本之间的梯度来确定调整方向。使用一种自适应的学习率调整策略来更新对抗样本"},{name:"ElasticNetL1",description:"基于L1范数正则化的弹性网络的对抗性攻击算法。使用了弹性网络的思想，结合了L1范数正则化正则化。它通过迭代优化的方式生成对抗样本"},{name:"ElasticNetL2",description:"使用弹性网络方法和L2范数正则化的对抗性攻击算法，使用了弹性网络的思想，结合了L2范数正则化。它通过迭代优化的方式生成对抗样本"}],[{name:"ElasticNet-EN",description:"使用弹性网络方法的L1和L2范数正则化的对抗性攻击算法,同时使用L1范数和L2范数正则化。L1范数正则化可以促使生成的对抗样本具有稀疏性，而L2范数正则化可以平衡对抗样本的整体扰动。这种结合可以帮助算法在生成对抗样本时取得更好的平衡，同时保持对抗样本的有效性和可解释性"},{name:"FeatureAdversaries",description:"针对输入数据中的特定特征的对抗性攻击算法，通过分析目标模型的预测行为和特征重要性，选择一个或多个目标特征进行攻击"},{name:"NewtonFool",description:"基于牛顿优化方法的对抗性攻击算法。使用牛顿优化方法来迭代地更新扰动值，利用了损失函数的二阶导数信息，通过求解方程来找到最小值。在每次迭代中，算法计算损失函数的梯度和海森矩阵，并使用这些信息来更新扰动值"}],[{name:"SpatialTransformation",description:"使用空间转换欺骗模型的对抗性攻击算法。对输入图像进行一系列的几何变换，例如旋转、缩放、平移等，以改变图像的外观和结构。这些变换会导致模型在处理经过转换的图像时产生错误的预测结果"},{name:"TargetedUniversalPerturbationL2",description:"利用L2范数正则化生成通用摄动的对抗性攻击算法，生成通用摄动，以欺骗目标模型。该算法使用L2范数正则化来生成摄动，以确保生成的摄动在图像中的扰动较小"},{name:"TargetedUniversalPerturbationLinf",description:"产生1-无穷范数正则化的通用摄动的对抗性攻击算法，生成通用摄动（universal perturbation），以欺骗目标模型。该算法使用1-无穷范数正则化来生成摄动，以确保生成的摄动在图像中的扰动较小"}],[{name:"VirtualAdversarialMethod",description:"基于虚拟对抗训练方法的对抗攻击算法。通过在原始样本的附近生成虚拟对抗样本，这些样本在输入空间中与原始样本非常接近，但在模型输出上具有最大化的不确定性。模型在原始样本和虚拟对抗样本之间进行训练，以增强模型对对抗性扰动的鲁棒性"},{name:"SignOPTAttack",description:"使用Sign-OPT方法进行优化的对抗性攻击算法"}]],selectedMethod:[],selectedAttributes:{},DefenseInfo:[{name:"基于知识图谱的自动化攻防测试",id:"graph",src:"",des:"基于不同任务下的对抗样本攻防算法之间的关系构建AI技战术知识图谱，并通过知识图谱生成自动化攻防测试方案，实现AI模型自动化攻防测试；"},{name:"规则驱动的自动化攻防测试方法",id:"rule",src:"",des:"在不同攻防场景中，基于预设规则匹配攻防参数，从而实现对于AI模型的高效测试；"},{name:"融合规则与知识图谱的自动化攻防测试",id:"graph_rule",src:"",des:"基于规则驱动的攻防参数匹配方法与基于知识图谱的攻防算法智能推荐方法对AI模型的鲁棒性进行测试"},{name:"基于流水线的自动化攻防测试",id:"flow",src:"",des:"基于流水线模式，从数据加载、模型加载、攻防训练、报告生成等环节对AI模型的鲁棒性进行测试"},{name:"融合规则与流水线的自动化攻防测试",id:"flow_rule",src:"",des:"基于规则驱动的攻防参数匹配方法与流水线模式的攻防测试策略对AI模型的鲁棒性进行测试"}],selectedDefense:"graph",ruleMethodInfo:[[{name:"FGSM",description:"Fast Gradient Sign MethodFGSM快速梯度符号法是一种简单而有效的生成对抗样本的方法，其工作方式如下：在给定输入数据后，利用已训练的模型输出预测并计算损失函数的梯度，然后使用梯度的符号来创建使损失最大化的新数据"},{name:"BIM",description:"Basic Iterative MethodBIM迭代式FGSM是对FGSM的改进方法，主要的改进有两点，其一是FGSM方法是一步完成的，而BIM方法通过多次迭代来寻找对抗样本；其次，为了避免迭代过程中出现超出有效值的情况出现，使用了一个修建方法严格限制像素值的范围"},{name:"RFGSM",description:"RFGSM算法：R+FGSM在FGSM中加入随机的步骤, 是一个在白盒设置下高效的能替代迭代攻击的方法"},{name:"FFGSM",description:"FFGSM算法：在使用FGSM攻击算法前加入随机初始化的扰动，经过实验发现基于FFGSM的对抗训练拥有高效性"},{name:"TIFGSM",description:"TIFGSM算法：Translation-Invarient FGSM将translation-invarient attack与FGSM结合，具体是将对抗目标调整为使像素平移后的对抗样本输出与真实标签的loss最大，同时使用卷积梯度的方法减小计算代价，使用该攻击方法拥有更强的迁移性"}],[{name:"MIFGSM",description:"MIFGSM算法：momentum iterative FGSM是一种使用momentum迭代梯度的方法，该方法在迭代梯度对抗攻击(如BIM)的基础上，累计每次梯度方向的速度向量作为momentum，每次对抗扰动不再直接使用梯度方向，转而采用momentum方向，从而稳定更新方向并避免局部极值，更好提高攻击迁移性"},{name:"DIFGSM",description:"DIFGSM算法：Diverse Inputs Iterative Fast Gradient Sign Method,通过创建多样的输入模式提高对抗样本的迁移性。做法是对输入的原图像以p的概率加上随机且可导的变换(transformation)，使用梯度的方法最大化模型对变换后的原图像的损失函数值从而得到对抗图像"},{name:"CW",description:"该方法的出发点是攻击比较有名的对抗样本防御方法-防御蒸馏(就防御蒸馏方法而言，它在基本的L-BFGS，FGSM攻击方法上表现本身就比较差)。对于寻找对抗样本过程中目标函数的设置将会极大的影响对抗样本的攻击效果，为此，通过目标函数的设定，在零范数，二范数和无穷范数的限制下分别设计了三种不同的寻找对抗样本的目标函数，这三种方法均可以绕过防御蒸馏的防御"},{name:"UPGD",description:"支持各种基于梯度的对抗性攻击选项的终极PGD。"},{name:"PGD",description:"Projected Gradient DescentPGD投影梯度下降法是FGSM的迭代版本，该方法思路和BIM基本相同，不同之处在于该方法在迭代过程中使用范数投影的方法来约束非法数据，并且相对于BIM有一个随机的开始噪声。支持L1 norm范数"}],[{name:"TPGD",description:"TPGD算法：基于KL-Divergence loss的pgd攻击"},{name:"PGDL2",description:"Projected Gradient DescentPGD投影梯度下降法是FGSM的迭代版本，该方法思路和BIM基本相同，不同之处在于该方法在迭代过程中使用范数投影的方法来约束非法数据，并且相对于BIM有一个随机的开始噪声。支持L2 norm范数"},{name:"SparseFool",description:"SparseFool算法：利用决策边界的低均值曲率计算对抗稀疏(sparse)扰动"},{name:"AutoPGD",description:"一种基于L1范数正则化的对抗性攻击算法，通过迭代生成对抗样本，以欺骗机器学习模型并导致错误的预测结果。该算法利用L1范数正则化来控制对抗样本的扰动大小，使得扰动更加稀疏"},{name:"OnePixel",description:" One pixel对抗攻击算法的特点为只改变一个像素点即可实现攻击，是一种黑盒攻击"}],[{name:"Square",description:"该方法的主体思路是使用优化理论中的随机搜索方法来寻找对抗噪声，这种随机搜索的方法很多黑盒攻击方法都采用过。随机搜索的总体框架大体相同，如下图所示。基本思路是在在一个采用空间中随机选择一个噪声，如果该噪声能够降低目标函数的损失则将其添加到原图片上，否则进入下一轮的随机搜索。支持Linf norm范数。"},{name:"DeepFool",description:"DeepFool方法的出发点是想要精确的度量模型对于对抗样本的鲁棒性，为此提出了鲁棒性定义和计算方法。最终使用该计算方法生成对抗样本"},{name:"FAB",description:"FAB算法：Fast Adaptive Boundary Attack使用类似DeepFool投影到决策平面的方法，主要不同的是，FAB将过程中的扰动尽可能偏向原样本点，从而可以找到能改变分类器输出的最小扰动。扰动的衡量距离为 linf"},{name:"Jitter",description:"Jitter算法：使用新的损失函数从而改善效率和攻击成功率，同时通过输出logits标准化到固定值范围将规模不变性引入损失函数"},{name:"MIFGSM",description:"MIFGSM算法：momentum iterative FGSM是一种使用momentum迭代梯度的方法，该方法在迭代梯度对抗攻击(如BIM)的基础上，累计每次梯度方向的速度向量作为momentum，每次对抗扰动不再直接使用梯度方向，转而采用momentum方向，从而稳定更新方向并避免局部极值，更好提高攻击迁移性"}]],selectedRuleMethod:[],buttonBGColor:{background:"#0B55F4",color:"#FFFFFF"},disStatus:!1,logflag:!1,percent:10,logtext:[],funcDesText:{name:"模型对抗性测试",imgpath:m.a,bgimg:h.a,destext:"自动化生成特定场景下的AI模型对抗测试方案",backinfo:"基于对抗样本攻击和防御算法之间的攻防关系，自动化生成特定场景下的AI模型对抗测试方案，评估AI模型鲁棒性。",highlight:["支持10类任务类型，如图片分类、目标检测、情感分析、机器翻译、3D点云、智能推荐、情感分析、语音识别、入侵检测、物体分割等机器学习任务","支持20多种攻防算法，如常见的FGSM、BIM、RFGSM、CW、PGD、TPGD、MIFGSM等攻击算法，InvGAN、SID、Defense-GAN等主流的防御算法","支持支持基于知识图谱的AI模型自动化攻防、基于流水线的AI模型自动化攻防、规则驱动的AI模型自动化攻防等5种安全自动化攻防技术"]},resultVisible:!1,result:{},postData:{},tid:"",stidlist:"",clk:"",logclk:""}},watch:{resultVisible:{immediate:!0,handler:function(e){e?this.noScroll():this.canScroll()}}},created:function(){document.title="模型对抗性测试"},methods:{closeDialog:function(){this.resultVisible=!1},updateMethod:function(e,t){1==t?this.selectedMethod.push(this.methodInfo[e].id):this.selectedMethod.splice(this.selectedMethod.indexOf(this.methodInfo[e].id),1),console.log(this.selectedMethod)},changeMethods:function(e,t){var a=document.getElementById("button"+e+t);""==a.style.color?(this.methodHoverIndex=e,this.methodDescription=this.methodInfoNoParam[e][t].description,a.style.color="#0B55F4",a.style.borderColor="#C8DCFB",a.style.background="#F2F4F9",this.selectedMethod.push(this.methodInfoNoParam[e][t].name),this.selectedAttributes[this.methodInfoNoParam[e][t].name]={}):(this.methodHoverIndex=-1,this.methodDescription="",a.style.color="",a.style.borderColor="#C8DCFB",a.style.background="#F2F4F9",a.blur(),this.selectedMethod.splice(this.selectedMethod.indexOf(this.methodInfoNoParam[e][t].name),1),delete this.selectedAttributes[this.methodInfoNoParam[e][t].name])},changeRuleMethods:function(e,t){var a=document.getElementById("button"+e+t);""==a.style.color?(this.methodHoverIndex=e,this.methodDescription=this.ruleMethodInfo[e][t].description,a.style.color="#0B55F4",a.style.borderColor="#C8DCFB",a.style.background="#F2F4F9",this.selectedRuleMethod.push(this.ruleMethodInfo[e][t].name),this.selectedAttributes[this.ruleMethodInfo[e][t].name]={}):(this.methodHoverIndex=-1,this.methodDescription="",a.style.color="",a.style.borderColor="#C8DCFB",a.style.background="#F2F4F9",a.blur(),this.selectedRuleMethod.splice(this.selectedRuleMethod.indexOf(this.ruleMethodInfo[e][t].name),1),delete this.selectedAttributes[this.ruleMethodInfo[e][t].name])},changeDataset:function(e){this.selectedDataset=e},changeModel:function(e){this.selectedModel=e},onMatchedMethodChange:function(e){console.log("radio checked",e.target.value)},updateDefense:function(e,t){this.selectedDefense=this.DefenseInfo[e].id,console.log(this.selectedDefense)},onFrameworkChange:function(e){console.log("radio checked",e.target.value)},exportResult:function(){if(confirm("您确认下载该pdf文件吗？")){var e=document.getElementById("download_page"),t={margin:[10,20,10,20],filename:this.tid+".pdf",image:{type:"jpeg",quality:1},html2canvas:{scale:5},jsPDF:{unit:"mm",format:"a4",orientation:"portrait"}};html2pdf().from(e).set(t).save()}},resultPro:function(e){},getData:function(){var e=this;e.$axios.get("/output/Resultdata",{params:{Taskid:e.tid}}).then(function(t){console.log("dataget:",t),e.result=t})},getLog:function(){var e=this;e.percent<99&&(e.percent+=1),e.$axios.get("/Task/QueryLog",{params:{Taskid:e.tid}}).then(function(t){if("{}"==i()(e.stidlist))e.logtext=[n()(t.data.Log).slice(-1)[0]];else for(var a in e.logtext=[],e.stidlist)e.logtext.push(t.data.Log[e.stidlist[a]])})},stopTimer:function(){1==this.result.data.stop&&this.tid==this.result.data.result.tid&&(this.percent=100,this.logflag=!1,clearInterval(this.clk),clearInterval(this.logclk),this.resultVisible=!0,this.result=this.result.data.result)},update:function(){this.getData();try{this.stopTimer()}catch(e){}},changeDefenseAlg:function(e){this.defendAlgorithm=e,console.log("defendAlgorithm",this.defendAlgorithm)},initParam:function(){this.logtext=[],this.percent=0,this.postData={},this.result={},this.tid="",this.stidlist={},""!=this.clk&&(window.clearInterval(this.clk),this.clk=""),""!=this.logclk&&(window.clearInterval(this.logclk),this.logclk="")},dataEvaClick:function(){this.initParam();var e=this.dataSetInfo[this.selectedDataset].name,t=this.modelInfo[this.selectedModel].name,a=this;this.$axios.post("/Task/CreateTask",{AttackAndDefenseTask:0}).then(function(s){if(console.log(s),a.tid=s.data.Taskid,"flow"==a.selectedDefense){if(0==a.selectedMethod.length)return void a.$message.warning("请至少选择一项对抗攻击方法！",3);for(var n=0;n<a.selectedMethod.length;n++)if(a.selectedMethod[n]in a.selectedAttributes)a.postData[a.selectedMethod[n]]=a.selectedAttributes[a.selectedMethod[n]];else{for(var r={},i=[],o=0;o<a.methodInfo.length;o++)if(a.methodInfo[o].id==a.selectedMethod[n]){i=a.methodInfo[o].attributes;break}for(var d=0;d<i.length;d++)for(var c=0;c<i[d].length;c++)"selectgroup"==i[d][c].type?r[i[d][c].key]=i[d][c].valuelist[i[d][c].defaultNumber]:r[i[d][c].key]=i[d][c].defaultNumber;a.postData[a.selectedMethod[n]]=r}a.postData.DatasetParam={name:e},a.postData.ModelParam={name:t,ckpt:null},a.postData.AutoMethod=a.selectedDefense,a.postData.AdvMethods=a.selectedMethod,a.postData.Taskid=a.tid}else["graph","graph_rule"].indexOf(a.selectedDefense)>-1?(a.postData.DatasetParam={name:e},a.postData.ModelParam={name:t,ckpt:null},a.postData.AutoMethod=a.selectedDefense,a.postData.Taskid=a.tid,a.postData.attack_mode=a.attackMode,a.postData.attack_type=a.attackType,a.postData.data_type=a.dataType,a.postData.defend_algorithm=a.defendAlgorithm):["rule","flow_rule"].indexOf(a.selectedDefense)>-1&&(a.postData.DatasetParam={name:e},a.postData.ModelParam={name:t,ckpt:null},a.postData.Taskid=a.tid,a.postData.AutoMethod=a.selectedDefense,a.postData.AdvMethods=a.selectedRuleMethod);console.log(a.postData),a.logclk=self.setInterval(a.getLog,300),a.$axios.post("/AutoAttack",a.postData).then(function(e){a.logflag=!0,a.stidlist={AutoAttack:e.data.stid},a.clk=self.setInterval(a.update,3e3)}).catch(function(e){console.log(e)})}).catch(function(e){console.log(e)})}}},S={render:function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",[a("a-layout",[a("a-layout-header",[a("navmodule")],1),e._v(" "),a("a-layout-content",[a("func_introduce",{attrs:{funcDesText:e.funcDesText}}),e._v(" "),a("div",{staticClass:"paramCon"},[a("h2",{staticClass:"subTitle",staticStyle:{"margin-top":"-96px"}},[e._v("参数配置")]),e._v(" "),a("div",{staticClass:"funcParam"},[a("div",{staticClass:"paramTitle"},[a("img",{staticClass:"paramIcom",attrs:{src:e.funcDesText.imgpath,alt:e.funcDesText.name}}),e._v(" "),a("h3",[e._v(e._s(e.funcDesText.name))]),e._v(" "),a("a-button",{staticClass:"DataEva",style:e.buttonBGColor,attrs:{disabled:e.disStatus},on:{click:e.dataEvaClick}},[a("a-icon",{attrs:{type:"security-scan"}}),e._v("\n                       评估\n                   ")],1)],1),e._v(" "),a("a-divider"),e._v(" "),a("div",{staticClass:"inputdiv"},[a("div",{staticClass:"mainParamNameNotop"},[e._v("请选择数据集")]),e._v(" "),e._l(e.dataSetInfo,function(t,s){return a("DataSetCard",e._b({key:"Dataset"+s,staticStyle:{width:"1104px","margin-bottom":"16px"},attrs:{indexInParent:s,checked:s==e.selectedDataset},on:{selectDataset:e.changeDataset}},"DataSetCard",t,!1))}),e._v(" "),a("div",{staticClass:"mainParamName48"},[e._v("请选择模型")]),e._v(" "),e._l(e.modelInfo,function(t,s){return a("ModelCard",e._b({key:"Model"+s,staticStyle:{width:"1104px","margin-bottom":"16px"},attrs:{indexInParent:s,checked:s==e.selectedModel},on:{selectModel:e.changeModel}},"ModelCard",t,!1))}),e._v(" "),a("div",{staticClass:"mainParamName48"},[e._v("请选择自动化攻防方案")]),e._v(" "),e._l(e.DefenseInfo,function(t,s){return a("DefenseCard",e._b({key:"Defense"+s,staticStyle:{width:"1104px"},attrs:{indexInParent:s,checked:e.selectedDefense==e.DefenseInfo[s].id},on:{selectedDefense:e.updateDefense}},"DefenseCard",t,!1))}),e._v(" "),"flow"==e.selectedDefense?a("div",[a("div",{staticClass:"mainParamName48"},[e._v("请选择攻击方法（可多选）")]),e._v(" "),e._l(e.methodInfo,function(t,s){return a("MethodCard",e._b({key:"Method"+s,staticStyle:{width:"1104px"},attrs:{indexInParent:s,attack_type:"advAttack",dataset:e.selectedDataset,checked:e.selectedMethod.indexOf(e.methodInfo[s].id)>-1},on:{updateAttributes:e.updataMethodAttributes,selectMethod:e.updateMethod}},"MethodCard",t,!1))}),e._v(" "),e._l(e.methodInfoNoParam,function(t,s){return a("div",{key:s,staticStyle:{"margin-bottom":"16px"}},[a("a-row",{staticStyle:{height:"50px"},attrs:{gutter:16,type:"flex"}},e._l(t,function(n,r){return a("a-col",{key:r,staticClass:"denfenseMethod",attrs:{flex:24/t.length}},[a("a-button",{attrs:{id:"button"+s+r},on:{click:function(t){return e.changeMethods(s,r)}}},[e._v(e._s(n.name))])],1)}),1),e._v(" "),e.methodHoverIndex==s&&""!==e.methodDescription?a("div",{staticStyle:{padding:"14px 24px","margin-bottom":"16px"}},[e._v(" "+e._s(e.methodDescription)+" ")]):e._e()],1)})],2):e._e(),e._v(" "),["graph","graph_rule"].indexOf(e.selectedDefense)>-1?a("div",[a("div",{staticClass:"attackMode"},[a("p",{staticClass:"mainParamName48"},[e._v("请选择攻击方式")]),e._v(" "),a("a-radio-group",{model:{value:e.attackMode,callback:function(t){e.attackMode=t},expression:"attackMode"}},[a("a-radio",{style:e.radioStyle,attrs:{value:"white_box"}},[e._v("\n                                    白盒攻击\n                                ")]),e._v(" "),a("a-radio",{style:e.radioStyle,attrs:{value:"black_box"}},[e._v("\n                                    黑盒攻击\n                                ")])],1)],1),e._v(" "),a("div",{staticClass:"attackType"},[a("p",{staticClass:"mainParamName48"},[e._v("请选择攻击类型")]),e._v(" "),a("a-radio-group",{model:{value:e.attackType,callback:function(t){e.attackType=t},expression:"attackType"}},[a("a-radio",{style:e.radioStyle,attrs:{value:"evasion_attack"}},[e._v("\n                                    逃逸攻击\n                                ")]),e._v(" "),a("a-radio",{style:e.radioStyle,attrs:{value:"poison_attack"}},[e._v("\n                                    毒化攻击\n                                ")])],1)],1),e._v(" "),a("div",{staticClass:"dataType"},[a("p",{staticClass:"mainParamName48"},[e._v("请选数据类型")]),e._v(" "),a("a-radio-group",{model:{value:e.dataType,callback:function(t){e.dataType=t},expression:"dataType"}},[a("a-radio",{style:e.radioStyle,attrs:{value:"image"}},[e._v("\n                                    图片\n                                ")]),e._v(" "),a("a-radio",{style:e.radioStyle,attrs:{value:"text"}},[e._v("\n                                    文本\n                                ")]),e._v(" "),a("a-radio",{style:e.radioStyle,attrs:{value:"graph"}},[e._v("\n                                    图\n                                ")])],1)],1),e._v(" "),a("div",{staticClass:"defenseAlg"},[a("p",{staticClass:"mainParamName48"},[e._v("请选择防御算法")]),e._v(" "),a("a-select",{staticStyle:{width:"500px"},attrs:{"default-value":"Adversarial-Training",value:e.defendAlgorithm},on:{change:e.changeDefenseAlg}},e._l(e.defenseList,function(t){return a("a-select-option",{key:t},[e._v("\n                                    "+e._s(t)+"\n                                ")])}),1)],1)]):e._e(),e._v(" "),["rule","flow_rule"].indexOf(e.selectedDefense)>-1?a("div",[a("div",{staticClass:"mainParamName48"},[e._v("请选择攻击方法（可多选）")]),e._v(" "),e._l(e.ruleMethodInfo,function(t,s){return a("div",{key:s,staticStyle:{"margin-bottom":"16px"}},[a("a-row",{staticStyle:{height:"50px"},attrs:{gutter:16,type:"flex"}},e._l(t,function(n,r){return a("a-col",{key:r,staticClass:"denfenseMethod",attrs:{flex:24/t.length}},[a("a-button",{attrs:{id:"button"+s+r},on:{click:function(t){return e.changeRuleMethods(s,r)}}},[e._v(e._s(n.name))])],1)}),1),e._v(" "),e.methodHoverIndex==s&&""!==e.methodDescription?a("div",{staticStyle:{padding:"14px 24px","margin-bottom":"16px"}},[e._v(" "+e._s(e.methodDescription)+" ")]):e._e()],1)})],2):e._e()],2)],1)]),e._v(" "),e.logflag?a("div",[a("showLog",{attrs:{percent:e.percent,logtext:e.logtext}})],1):e._e(),e._v(" "),a("autoAdvAttackEval",{attrs:{"is-show":e.resultVisible,result:e.result,postData:e.postData},on:{"on-close":function(){e.resultVisible=!e.resultVisible}}})],1),e._v(" "),a("a-layout-footer")],1)],1)},staticRenderFns:[]};var D=a("VU/8")(k,S,!1,function(e){a("RzuM")},"data-v-76b0b111",null);t.default=D.exports},f4au:function(e,t,a){"use strict";var s={props:{name:{default:"Nash博弈"},src:{default:a("JdmF")},des:"",checked:{default:!0},indexInParent:0},methods:{selectedDefense:function(){this.$emit("selectedDefense",this.indexInParent,!this.checked)}}},n={render:function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",[a("a-radio",{staticStyle:{width:"100%"},attrs:{checked:e.checked},on:{click:e.selectedDefense}},[e._v(e._s(e.name))]),e._v(" "),e.checked?a("div",{staticClass:"describe_con"},[a("p",[e._v(e._s(e.des))])]):e._e()],1)},staticRenderFns:[]};var r=a("VU/8")(s,n,!1,function(e){a("r0Vr")},"data-v-571224dc",null);t.a=r.exports},nxKd:function(e,t){e.exports=[{name:"逃逸攻击",desc:"攻击模式",paper:"",method_type:""},{name:"毒化攻击",desc:"攻击模式",paper:"",method_type:""},{name:"图片",desc:"攻击数据",paper:"",method_type:""},{name:"文本",desc:"攻击数据",paper:"",method_type:""},{name:"图",desc:"攻击数据",paper:"",method_type:""},{name:"黑盒",desc:"攻击场景",paper:"",method_type:""},{name:"白盒",desc:"攻击场景",paper:"",method_type:""},{name:"L-BFGS",desc:"攻击算法",paper:"Intriguing properties of neural networks",method_type:"基于优化"},{name:"FGSM",desc:"攻击算法",paper:"Explaining and harnessing adversarial examples",method_type:"基于梯度"},{name:"DeepFool",desc:"攻击算法",paper:"Deepfool: a simple and accurate method to fool deep neural networks",method_type:"基于超平面"},{name:"C&W",desc:"攻击算法",paper:"Towards neural networks",method_type:"基于优化"},{name:"BIM",desc:"攻击算法",paper:"Adversarial machine learning at scale",method_type:"基于梯度"},{name:"Adversarial Patch",desc:"攻击算法",paper:"Adversarial patch",method_type:"对抗补丁"},{name:"ATN",desc:"攻击算法",paper:"Adversarial Transformation Networks",method_type:"基于生成模型"},{name:"JSMA",desc:"攻击算法",paper:"The limitations of deep learning in adversarial settings",method_type:"基于梯度"},{name:"PGD",desc:"攻击算法",paper:"Towards deep learning models resistant to adversarial attacks",method_type:"基于梯度"},{name:"TPGD",desc:"攻击算法",paper:"Theoretically Principled Trade-off between Robustness and Accuracy",method_type:"基于梯度"},{name:"AutoPGD",desc:"攻击算法",paper:"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",method_type:"基于梯度"},{name:"DIFGSM",desc:"攻击算法",paper:"Improving Transferability of Adversarial Examples with Input Diversity ",method_type:"基于梯度"},{name:"RFGSM",desc:"攻击算法",paper:"Ensemble Adversarial Traning: Attacks and Defences",method_type:"基于梯度"},{name:"FAB",desc:"攻击算法",paper:"Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack",method_type:"基于决策边界"},{name:"ZOO",desc:"攻击算法",paper:"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models",method_type:"基于梯度估计"},{name:"Square",desc:"攻击算法",paper:"Square Attack: a query-efficient black-box adversarial attack via random search ",method_type:"基于梯度"},{name:"AutoZOOM",desc:"攻击算法",paper:"Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks",method_type:"基于梯度估计"},{name:"Boundary-Attack",desc:"攻击算法",paper:"Decision-based adversarial attacks: Reliable attacks against black-box machine learning \nmodels",method_type:"基于决策"},{name:"Substitute-Model",desc:"攻击算法",paper:"Practical black-box attacks against machine learning",method_type:"基于替代模型"},{name:"NES-Attack",desc:"攻击算法",paper:"Black-box Adversarial Attacks with Limited Queries and Information",method_type:"基于梯度估计"},{name:"SimBa",desc:"攻击算法",paper:"Simple Black-box Adversarial Attacks",method_type:"基于梯度估计"},{name:"DeepConfuse",desc:"攻击算法",paper:"Learning to confuse: Generating training time adversarial data with auto-encoder",method_type:"投毒攻击"},{name:"NNPoision",desc:"攻击算法",paper:"Generative Poisoning Attack Method Against Neural Networks",method_type:"投毒攻击"},{name:"Badnets",desc:"攻击算法",paper:"Badnets: Identifying vulnerabilities in the machine learning model supply chain",method_type:"投毒攻击"},{name:"Trojan Attack",desc:"攻击算法",paper:"Trojaning attack on neural networks",method_type:"投毒攻击"},{name:"Bullseye Polytope",desc:"攻击算法",paper:"Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability",method_type:"投毒攻击"},{name:"PolytopeAttack",desc:"攻击算法",paper:"Transferable Clean-Label Poisoning Attacks on Deep Neural Nets",method_type:"投毒攻击"},{name:"BPDA",desc:"攻击算法",paper:"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",method_type:"基于梯度"},{name:"MI-FGSM",desc:"攻击算法",paper:"Boosting Adversarial Attacks with Momentum",method_type:"基于梯度"},{name:"Adversarial-Training",desc:"防御算法",paper:"Explaining and harnessing adversarial examples",method_type:"对抗训练"},{name:"IBP-CROWN",desc:"防御算法",paper:"Towards stable and efficient training of verifiably robust neural networks",method_type:"形式化验证"},{name:"Defense-GAN",desc:"防御算法",paper:"Defense-gan: Protecting classifiers against adversarial attacks using generative models",method_type:"图像预处理"},{name:"InvGAN",desc:"防御算法",paper:"Invert and Defend: Model-based Approximate Inversion of Generative Adversarial Networks for Secure Inference",method_type:"图像预处理"},{name:"SID",desc:"防御算法",paper:"Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial-Transform Domain",method_type:"对抗样本检测"},{name:"ADC-detector",desc:"防御算法",paper:"ACT-Detector: Adaptive channel transformation-based light-weighted detector for adversarial attacks",method_type:"对抗样本检测"},{name:"DIP",desc:"防御算法",paper:"Real-Time Adversarial Attack Detection with Deep Image Prior Initialized as a High-Level Representation Based Blurring Network",method_type:"对抗样本检测"},{name:"SmsNet",desc:"防御算法",paper:"SmsNet: A New Deep Convolutional Neural Network Model for Adversarial Example Detection",method_type:"对抗样本检测"},{name:"Activation Clustering",desc:"防御算法",paper:"Detecting backdoor attacks on deep neural networks by activation clustering",method_type:"后门攻击检测"},{name:"Neural Cleanse",desc:"防御算法",paper:"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks",method_type:"后门攻击检测"},{name:"Deep k-NN",desc:"防御算法",paper:"Deep k-nn defense against clean-label data poisoning attacks",method_type:"后门攻击检测"},{name:"DeepWordBug",desc:"攻击算法",paper:"Black-box generation of adversarial text sequences to evade deep learning classifiers",method_type:"关键单词或字母替换"},{name:"BERT-Attack",desc:"攻击算法",paper:"Bert-attack: Adversarial attack against bert using bert",method_type:"实用Bert攻击"},{name:"WordChange",desc:"攻击算法",paper:"Wordchange: Adversarial examples generation approach for chinese text classification",method_type:"关键单词或字母替换"},{name:"BAE",desc:"攻击算法",paper:"Bae: Bert-based adversarial examples for text classification",method_type:"关键单词替换"},{name:"TextFooler",desc:"攻击算法",paper:"Is bert really robust? a strong baseline for natural language attack on text classification and entailment",method_type:"关键单词替换"},{name:"CLARE",desc:"攻击算法",paper:"Contextualized perturbation for textual adversarial attack",method_type:"关键单词替换"},{name:"AEG",desc:"攻击算法",paper:"Generating black-box adversarial examples for text classifiers using a deep reinforced model",method_type:"关键单词替换"},{name:"Hard-Label-Attack",desc:"攻击算法",paper:"Generating natural language attacks in a hard label black box setting",method_type:"同意词替换"},{name:"Lage-Scale-Adversarial-Attack",desc:"攻击算法",paper:"Generating natural language adversarial examples on a large scale with generative models",method_type:"生成模型"},{name:"Robust-Word-Recognition",desc:"防御算法",paper:"Combating adversarial misspellings with robust word recognition",method_type:"语法检查"},{name:"DISP",desc:"防御算法",paper:"Learning to discriminate perturbations for blocking adversarial attacks in text classification",method_type:"数据转换"},{name:"AMDA",desc:"防御算法",paper:"Better robustness by more coverage: Adversarial training with mixup augmentation for robust fine-tuning",method_type:"鲁棒性训练"},{name:"Safer",desc:"防御算法",paper:"SAFER: A structure-free approach for certified robustness to adversarial word substitutions",method_type:"形式化验证"},{name:"FGWS",desc:"防御算法",paper:"Frequency-guided word substitutions for detecting textual adversarial examples",method_type:"对抗样本检测"},{name:"RanMask",desc:"防御算法",paper:"Certified Robustness to Text Adversarial Attacks by Randomized [MASK]",method_type:"形式化验证"},{name:"TextFirewall",desc:"防御算法",paper:"TextFirewall: Omni-Defending Against Adversarial Texts in Sentiment Classification",method_type:"对抗样本检测"},{name:"BAT",desc:"防御算法",paper:"Adversarial training for aspect-based sentiment analysis with bert",method_type:"数据转换"},{name:"PWWS",desc:"攻击算法",paper:"Frequency-guided word substitutions for detecting textual adversarial examples",method_type:""},{name:"NIPA",desc:"攻击算法",paper:"Adversarial attacks on graph neural networks via node injections: A hierarchical reinforcement learning approach",method_type:""},{name:"PoisonRec",desc:"攻击算法",paper:"Poisonrec: an adaptive data poisoning framework for attacking black-box recommender systems",method_type:""},{name:"GF-Attack",desc:"攻击算法",paper:"Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge[",method_type:""},{name:"TNA",desc:"攻击算法",paper:"Influence function based data poisoning attacks to top-n recommender systems",method_type:""},{name:"CopyAttack",desc:"攻击算法",paper:"Attacking Black-box Recommendations via Copying Cross-domain User Profiles",method_type:""},{name:"Single-Node-Attack",desc:"攻击算法",paper:"Single-Node Attack for Fooling Graph Neural Networks",method_type:""},{name:"TrialAttack",desc:"攻击算法",paper:"Triple Adversarial Learning for Influence based Poisoning Attack in Recommender Systems",method_type:""},{name:"AMR",desc:"防御算法",paper:"Adversarial training towards robust multimedia recommender system",method_type:"鲁棒性训练"},{name:"FNCF",desc:"防御算法",paper:"Enhancing the robustness of neural collaborative filtering systems under malicious attacks",method_type:""},{name:"APT",desc:"防御算法",paper:"Fight Fire with Fire: Towards Robust Recommender Systems via Adversarial Poisoning Training",method_type:"鲁棒性训练"},{name:"DAVE",desc:"防御算法",paper:"Dual Adversarial Variational Embedding for Robust Recommendation",method_type:"鲁棒性训练"},{name:"SAO",desc:"防御算法",paper:"Sequential-based adversarial optimisation for personalised top-n item recommendation",method_type:"鲁棒性训练"},{name:"ATMBPR",desc:"防御算法",paper:"Adversarial training-based mean Bayesian personalized ranking for recommender system",method_type:"鲁棒性训练"}]},r0Vr:function(e,t){},r1Wa:function(e,t){e.exports=[{source:"图片",dest:"L-BFGS",desc:""},{source:"白盒",dest:"L-BFGS",desc:""},{source:"逃逸攻击",dest:"L-BFGS",desc:""},{source:"图片",dest:"FGSM",desc:""},{source:"白盒",dest:"FGSM",desc:""},{source:"逃逸攻击",dest:"FGSM",desc:""},{source:"图片",dest:"DeepFool",desc:""},{source:"白盒",dest:"DeepFool",desc:""},{source:"逃逸攻击",dest:"DeepFool",desc:""},{source:"图片",dest:"C&W",desc:""},{source:"白盒",dest:"C&W",desc:""},{source:"逃逸攻击",dest:"C&W",desc:""},{source:"图片",dest:"BIM",desc:""},{source:"白盒",dest:"BIM",desc:""},{source:"逃逸攻击",dest:"BIM",desc:""},{source:"图片",dest:"Adversarial Patch",desc:""},{source:"白盒",dest:"Adversarial Patch",desc:""},{source:"逃逸攻击",dest:"Adversarial Patch",desc:""},{source:"图片",dest:"ATN",desc:""},{source:"白盒",dest:"ATN",desc:""},{source:"逃逸攻击",dest:"ATN",desc:""},{source:"图片",dest:"JSMA",desc:""},{source:"白盒",dest:"JSMA",desc:""},{source:"逃逸攻击",dest:"JSMA",desc:""},{source:"图片",dest:"PGD",desc:""},{source:"白盒",dest:"PGD",desc:""},{source:"逃逸攻击",dest:"PGD",desc:""},{source:"图片",dest:"TPGD",desc:""},{source:"白盒",dest:"TPGD",desc:""},{source:"逃逸攻击",dest:"TPGD",desc:""},{source:"图片",dest:"AutoPGD",desc:""},{source:"白盒",dest:"AutoPGD",desc:""},{source:"逃逸攻击",dest:"AutoPGD",desc:""},{source:"图片",dest:"DIFGSM",desc:""},{source:"白盒",dest:"DIFGSM",desc:""},{source:"逃逸攻击",dest:"DIFGSM",desc:""},{source:"图片",dest:"RFGSM",desc:""},{source:"白盒",dest:"RFGSM",desc:""},{source:"逃逸攻击",dest:"RFGSM",desc:""},{source:"图片",dest:"FAB",desc:""},{source:"白盒",dest:"FAB",desc:""},{source:"逃逸攻击",dest:"FAB",desc:""},{source:"图片",dest:"ZOO",desc:""},{source:"黑盒",dest:"ZOO",desc:""},{source:"逃逸攻击",dest:"ZOO",desc:""},{source:"图片",dest:"Square",desc:""},{source:"黑盒",dest:"Square",desc:""},{source:"逃逸攻击",dest:"Square",desc:""},{source:"图片",dest:"AutoZOOM",desc:""},{source:"黑盒",dest:"AutoZOOM",desc:""},{source:"逃逸攻击",dest:"AutoZOOM",desc:""},{source:"图片",dest:"Boundary-Attack",desc:""},{source:"黑盒",dest:"Boundary-Attack",desc:""},{source:"逃逸攻击",dest:"Boundary-Attack",desc:""},{source:"图片",dest:"Substitute-Model",desc:""},{source:"黑盒",dest:"Substitute-Model",desc:""},{source:"逃逸攻击",dest:"Substitute-Model",desc:""},{source:"图片",dest:"NES-Attack",desc:""},{source:"黑盒",dest:"NES-Attack",desc:""},{source:"逃逸攻击",dest:"NES-Attack",desc:""},{source:"图片",dest:"SimBa",desc:""},{source:"黑盒",dest:"SimBa",desc:""},{source:"逃逸攻击",dest:"SimBa",desc:""},{source:"图片",dest:"DeepConfuse",desc:""},{source:"白盒",dest:"DeepConfuse",desc:""},{source:"毒化攻击",dest:"DeepConfuse",desc:""},{source:"图片",dest:"NNPoision",desc:""},{source:"白盒",dest:"NNPoision",desc:""},{source:"毒化攻击",dest:"NNPoision",desc:""},{source:"图片",dest:"Badnets",desc:""},{source:"白盒",dest:"Badnets",desc:""},{source:"毒化攻击",dest:"Badnets",desc:""},{source:"图片",dest:"Trojan Attack",desc:""},{source:"白盒",dest:"Trojan Attack",desc:""},{source:"毒化攻击",dest:"Trojan Attack",desc:""},{source:"图片",dest:"Bullseye Polytope",desc:""},{source:"黑盒",dest:"Bullseye Polytope",desc:""},{source:"Bullseye Polytope",dest:"Deep k-NN",desc:"攻击"},{source:"毒化攻击",dest:"Bullseye Polytope",desc:""},{source:"图片",dest:"PolytopeAttack",desc:""},{source:"黑盒",dest:"PolytopeAttack",desc:""},{source:"毒化攻击",dest:"PolytopeAttack",desc:""},{source:"图片",dest:"BPDA",desc:""},{source:"白盒",dest:"BPDA",desc:""},{source:"BPDA",dest:"Adversarial-Training",desc:"攻击"},{source:"逃逸攻击",dest:"BPDA",desc:""},{source:"图片",dest:"MI-FGSM",desc:""},{source:"白盒",dest:"MI-FGSM",desc:""},{source:"逃逸攻击",dest:"MI-FGSM",desc:""},{source:"图片",dest:"Adversarial-Training",desc:""},{source:"Adversarial-Training",dest:"FGSM",desc:"防御"},{source:"Adversarial-Training",dest:"C&W",desc:"防御"},{source:"图片",dest:"IBP-CROWN",desc:""},{source:"图片",dest:"Defense-GAN",desc:""},{source:"Defense-GAN",dest:"FGSM",desc:"防御"},{source:"Defense-GAN",dest:"C&W",desc:"防御"},{source:"图片",dest:"InvGAN",desc:""},{source:"InvGAN",dest:"FGSM",desc:"防御"},{source:"InvGAN",dest:"C&W",desc:"防御"},{source:"InvGAN",dest:"BPDA",desc:"防御"},{source:"图片",dest:"SID",desc:""},{source:"SID",dest:"DeepFool",desc:"防御"},{source:"SID",dest:"FGSM",desc:"防御"},{source:"SID",dest:"BIM",desc:"防御"},{source:"SID",dest:"C&W",desc:"防御"},{source:"图片",dest:"ADC-detector",desc:""},{source:"ADC-detector",dest:"FGSM",desc:"防御"},{source:"ADC-detector",dest:"MI-FGSM",desc:"防御"},{source:"ADC-detector",dest:"DeepFool",desc:"防御"},{source:"ADC-detector",dest:"C&W",desc:"防御"},{source:"图片",dest:"DIP",desc:""},{source:"DIP",dest:"FGSM",desc:"防御"},{source:"DIP",dest:"BIM",desc:"防御"},{source:"DIP",dest:"C&W",desc:"防御"},{source:"图片",dest:"SmsNet",desc:""},{source:"SmsNet",dest:"FGSM",desc:"防御"},{source:"SmsNet",dest:"DeepFool",desc:"防御"},{source:"SmsNet",dest:"C&W",desc:"防御"},{source:"图片",dest:"Activation Clustering",desc:""},{source:"Activation Clustering",dest:"Badnets",desc:"防御"},{source:"图片",dest:"Neural Cleanse",desc:""},{source:"Neural Cleanse",dest:"Badnets",desc:"防御"},{source:"Neural Cleanse",dest:"Trojan Attack",desc:"防御"},{source:"图片",dest:"Deep k-NN",desc:""},{source:"Deep k-NN",dest:"PolytopeAttack",desc:"防御"},{source:"文本",dest:"DeepWordBug",desc:""},{source:"黑盒",dest:"DeepWordBug",desc:""},{source:"逃逸攻击",dest:"DeepWordBug",desc:""},{source:"文本",dest:"BERT-Attack",desc:""},{source:"黑盒",dest:"BERT-Attack",desc:""},{source:"逃逸攻击",dest:"BERT-Attack",desc:""},{source:"文本",dest:"WordChange",desc:""},{source:"黑盒",dest:"WordChange",desc:""},{source:"逃逸攻击",dest:"WordChange",desc:""},{source:"文本",dest:"BAE",desc:""},{source:"黑盒",dest:"BAE",desc:""},{source:"逃逸攻击",dest:"BAE",desc:""},{source:"文本",dest:"TextFooler",desc:""},{source:"黑盒",dest:"TextFooler",desc:""},{source:"逃逸攻击",dest:"TextFooler",desc:""},{source:"文本",dest:"CLARE",desc:""},{source:"黑盒",dest:"CLARE",desc:""},{source:"逃逸攻击",dest:"CLARE",desc:""},{source:"文本",dest:"AEG",desc:""},{source:"黑盒",dest:"AEG",desc:""},{source:"逃逸攻击",dest:"AEG",desc:""},{source:"文本",dest:"Hard-Label-Attack",desc:""},{source:"黑盒",dest:"Hard-Label-Attack",desc:""},{source:"逃逸攻击",dest:"Hard-Label-Attack",desc:""},{source:"文本",dest:"Lage-Scale-Adversarial-Attack",desc:""},{source:"白盒",dest:"Lage-Scale-Adversarial-Attack",desc:""},{source:"逃逸攻击",dest:"Lage-Scale-Adversarial-Attack",desc:""},{source:"文本",dest:"Robust-Word-Recognition",desc:""},{source:"文本",dest:"DISP",desc:""},{source:"文本",dest:"AMDA",desc:""},{source:"AMDA",dest:"TextFooler",desc:"防御"},{source:"AMDA",dest:"PWWS",desc:"防御"},{source:"文本",dest:"Safer",desc:""},{source:"Safer",dest:"Genetic-Attack",desc:"防御"},{source:"文本",dest:"FGWS",desc:""},{source:"FGWS",dest:"PWWS",desc:"防御"},{source:"文本",dest:"RanMask",desc:""},{source:"RanMask",dest:"TextFooler",desc:"防御"},{source:"RanMask",dest:"Bert-Attack",desc:"防御"},{source:"RanMask",dest:"DeepWordBug",desc:"防御"},{source:"文本",dest:"TextFirewall",desc:""},{source:"TextFirewall",dest:"DeepWordBug",desc:"防御"},{source:"TextFirewall",dest:"PWWS",desc:"防御"},{source:"文本",dest:"BAT",desc:""},{source:"BAT",dest:"Gradient-Attack",desc:"防御"},{source:"文本",dest:"PWWS",desc:""},{source:"白盒攻击",dest:"PWWS",desc:""},{source:"逃逸攻击",dest:"PWWS",desc:""},{source:"图",dest:"NIPA",desc:""},{source:"毒化攻击",dest:"NIPA",desc:""},{source:"图",dest:"PoisonRec",desc:""},{source:"毒化攻击",dest:"PoisonRec",desc:""},{source:"图",dest:"GF-Attack",desc:""},{source:"毒化攻击",dest:"GF-Attack",desc:""},{source:"图",dest:"TNA",desc:""},{source:"毒化攻击",dest:"TNA",desc:""},{source:"图",dest:"CopyAttack",desc:""},{source:"毒化攻击",dest:"CopyAttack",desc:""},{source:"图",dest:"Single-Node-Attack",desc:""},{source:"毒化攻击",dest:"Single-Node-Attack",desc:""},{source:"图",dest:"TrialAttack",desc:""},{source:"毒化攻击",dest:"TrialAttack",desc:""},{source:"图",dest:"AMR",desc:""},{source:"图",dest:"FNCF",desc:""},{source:"图",dest:"APT",desc:""},{source:"APT",dest:"AUSH",desc:"防御"},{source:"APT",dest:"TNA",desc:"防御"},{source:"APT",dest:"PGA",desc:"防御"},{source:"图",dest:"DAVE",desc:""},{source:"DAVE",dest:"AAE",desc:"防御"},{source:"图",dest:"SAO",desc:""},{source:"图",dest:"ATMBPR",desc:""}]}});