{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import scipy.special\n",
    "MALE = 1\n",
    "FEMALE = 0\n",
    "NO_DOMAIN = 2\n",
    "sigmoid = scipy.special.expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimize_potentials_given_known_domain():\n",
    "    \"\"\"Given a set of network outputs on a test set, updates potentials to reduce bias.\n",
    "    \n",
    "    Args:\n",
    "      input_potentials: A float64 numpy array with shape (test_set_size, class_count).\n",
    "        Contains the network outputs on each test example, for a single class prediction\n",
    "        with known domain.\n",
    "      gt_domain: An int32 numpy array with shape (test_set_size,). The ground truth\n",
    "        domain. Used to do the optimization.\n",
    "      gt_class: An int32 numpy array with shape (test_set_size,). The ground truth\n",
    "        class label. Used only to compute accuracy.\n",
    "      training_set_frequencies: A float32 numpy array with shape (test_set_size, class_count).\n",
    "        The relative frequencies of the training set classes given each example's known domain.\n",
    "        \n",
    "    Returns:\n",
    "      output_potentials: A float64 numpy array with shape (test_set_size, class_count).\n",
    "        Contains the optimized network potentials that are the result of the optimization.\n",
    "      output_predictions: An int32 numpy array with shape (test_set_size,). Contains\n",
    "        the final network predictions (i.e. just an argmax over the potentials).\n",
    "    \"\"\"\n",
    "    def __init__(self, gt_labels, gt_domain, lr, margin, apply_prior_shift,\n",
    "                 inputs_are_activations, method_name, target_domain_ratios,\n",
    "                 domain_labels, inference_thresholds, training_set_targets, verbosity=2,\n",
    "                 total_epochs=100):\n",
    "        self.test_set_size = gt_labels.shape[0]\n",
    "        self.class_count = gt_labels.shape[1] // 2\n",
    "        self.gt_labels = gt_labels\n",
    "        self.gt_domain = gt_domain\n",
    "        self.lr = lr\n",
    "        self.margin = margin\n",
    "        self.apply_prior_shift = apply_prior_shift\n",
    "        self.inputs_are_activations = inputs_are_activations\n",
    "        self.method_name = method_name\n",
    "        self.target_domain_ratios = target_domain_ratios\n",
    "        self.domain_labels = domain_labels\n",
    "        self.verbosity = verbosity\n",
    "        self.inference_thresholds = inference_thresholds\n",
    "        self.training_set_targets = training_set_targets\n",
    "        gt_labels_bool = gt_labels.astype(np.bool)\n",
    "        self.gt_class = (gt_labels_bool[:, :self.class_count] | gt_labels_bool[:, self.class_count:]).astype(np.int32)\n",
    "        self.total_epochs = total_epochs\n",
    "    \n",
    "    def multiclass_probabilities(self, potentials):\n",
    "        \"\"\"Returns the probability of each class from the network activation.\"\"\"\n",
    "        if self.inputs_are_activations:\n",
    "            return sigmoid(potentials)\n",
    "        return potentials\n",
    "\n",
    "    def compute_sample_weights(self):\n",
    "        n_m = np.sum(self.gt_labels[:, :self.class_count], axis=0).astype(np.float64)\n",
    "        n_w = np.sum(self.gt_labels[:, self.class_count:], axis=0).astype(np.float64)\n",
    "        male_class_weights = (n_m + n_w) / (2.0*n_m)\n",
    "        female_class_weights = (n_m + n_w) / (2.0*n_w)\n",
    "        sample_weights = np.zeros_like(self.gt_labels, dtype=np.float64)\n",
    "        sample_weights[:, :self.class_count] = np.tile(np.reshape(male_class_weights,\n",
    "                                                                  (1, self.class_count)),\n",
    "                                                       (self.gt_labels.shape[0], 1))\n",
    "        sample_weights[:, self.class_count:] = np.tile(np.reshape(female_class_weights,\n",
    "                                                                  (1, self.class_count)),\n",
    "                                                       (self.gt_labels.shape[0], 1))\n",
    "        sample_weights[self.gt_labels == 0] = 1.0\n",
    "        collapsed = np.ones((self.test_set_size, self.class_count), dtype=np.float64)\n",
    "        collapsed[self.gt_domain == MALE, :] = sample_weights[self.gt_domain == MALE, :self.class_count]\n",
    "        collapsed[self.gt_domain == FEMALE, :] = sample_weights[self.gt_domain == FEMALE, self.class_count:]\n",
    "        sample_weights = collapsed\n",
    "        return sample_weights\n",
    "    \n",
    "    def compute_mAP(self, potentials):\n",
    "        probs = self.multiclass_probabilities(potentials)\n",
    "        sample_weights = self.compute_sample_weights()\n",
    "        APs = [sklearn.metrics.average_precision_score(self.gt_class[:, i], probs[:, i],\n",
    "                                                       sample_weight=sample_weights[:, i])\n",
    "               for i in range(self.class_count)]\n",
    "        return 100.0 * np.mean(APs)\n",
    "    \n",
    "    def compute_accuracy(self, potentials):\n",
    "        decisions = self.multiclass_inference(potentials)\n",
    "        return 100.0 * np.mean(decisions == self.gt_class)\n",
    "    \n",
    "    def compute_bias(self, decisions):\n",
    "        if decisions.shape[1] == self.class_count:\n",
    "            domain_decisions = np.zeros((self.test_set_size, 2*self.class_count), dtype=np.float64)\n",
    "            for i in range(decisions.shape[0]):\n",
    "                g = int(self.gt_domain[i])\n",
    "                if g == NO_DOMAIN:\n",
    "                    continue\n",
    "                domain_decisions[i, g*self.class_count:(g+1)*self.class_count] = decisions[i, :]\n",
    "            decisions = domain_decisions\n",
    "        class_domain_counts = np.sum(decisions, axis=0)\n",
    "        class_counts = class_domain_counts[:self.class_count] + class_domain_counts[self.class_count:]\n",
    "        class_counts = np.tile(class_counts, 2)\n",
    "        domain_weights = np.divide(class_domain_counts, class_counts,\n",
    "                                  out=np.zeros_like(class_counts), where=(class_counts != 0.0))\n",
    "        return domain_weights\n",
    "\n",
    "    def compute_bias_amplification(self, potentials):\n",
    "        decisions = self.multiclass_inference(potentials).astype(np.float64)\n",
    "        test_bias = self.compute_bias(decisions)\n",
    "        train_bias = self.compute_bias(self.training_set_targets)\n",
    "        amplified_bias = np.abs(test_bias - train_bias)\n",
    "        mAB = np.mean(amplified_bias)\n",
    "        return mAB\n",
    "    \n",
    "    def multiclass_count_domain_incidence_from_gt(self, predictions):\n",
    "        male_gt_rows = predictions[self.gt_domain == MALE, :]\n",
    "        female_gt_rows = predictions[self.gt_domain == FEMALE, :]\n",
    "        male_gt_count = np.sum(male_gt_rows, axis=0)\n",
    "        female_gt_count = np.sum(female_gt_rows, axis=0)\n",
    "        count_per_class = np.stack([male_gt_count, female_gt_count], axis=1).astype(np.float64)\n",
    "        return count_per_class\n",
    "    \n",
    "    def multiclass_inference(self, potentials):\n",
    "        \"\"\"Converts the potentials into decisions.\"\"\"\n",
    "        probs = self.multiclass_probabilities(potentials)\n",
    "        thresholds = np.tile(self.inference_thresholds, (self.test_set_size, 1))\n",
    "        decisions = (probs > thresholds).astype(np.int32)\n",
    "        return decisions\n",
    "\n",
    "    def generate_constraints(self):\n",
    "        constraints = np.zeros((self.class_count, 2, 2))\n",
    "        constraints[:, 0, 0] = self.target_domain_ratios - 1 - self.margin\n",
    "        constraints[:, 0, 1] = self.target_domain_ratios - self.margin\n",
    "        constraints[:, 1, 0] = 1 - (self.margin + self.target_domain_ratios)\n",
    "        constraints[:, 1, 1] = -(self.margin + self.target_domain_ratios)\n",
    "        return constraints\n",
    "\n",
    "    def optimize(self, input_potentials):\n",
    "        if self.verbosity >= 1:\n",
    "            initial_mAP = self.compute_mAP(input_potentials)\n",
    "            initial_bias = np.mean(np.abs(0.5 - self.compute_bias(input_potentials)))\n",
    "            initial_bias_amplification = self.compute_bias_amplification(input_potentials)\n",
    "            name_in = ('%s, before optimization' % self.method_name).ljust(85)\n",
    "            print('%s mAP. %0.2f%%. Bias %0.3f' % (name_in, initial_mAP, initial_bias))\n",
    "            print('\\t bias amplification: %0.4f. ' % initial_bias_amplification)\n",
    "        if self.verbosity >= 2:\n",
    "            initial_mAP = self.compute_mAP(input_potentials)\n",
    "            print('Pre optimization mAP: %0.2f%%' % initial_mAP)\n",
    "        lambdas = np.zeros((self.class_count, 2), dtype=np.float64)\n",
    "        current_potentials = input_potentials.copy()\n",
    "        constraints = self.generate_constraints()\n",
    "        initial_predictions = self.multiclass_inference(input_potentials)\n",
    "        for epoch in range(self.total_epochs):\n",
    "            violated_constraint_count = 0\n",
    "            error = np.zeros((self.class_count, 2), dtype=np.float64)\n",
    "\n",
    "            predictions = self.multiclass_inference(current_potentials)\n",
    "            count_per_class = self.multiclass_count_domain_incidence_from_gt(predictions)\n",
    "            count_per_class = np.reshape(count_per_class, [self.class_count, 1, 2])\n",
    "            constraint_delta = np.sum(constraints * count_per_class, axis=2)\n",
    "            lambdas += self.lr * constraint_delta\n",
    "            error += constraint_delta\n",
    "            count_per_class = np.reshape(count_per_class, [self.class_count, 2])\n",
    "\n",
    "            lambdas = np.maximum(lambdas, 0)\n",
    "            violated_constraint_count = np.count_nonzero(error > 0)\n",
    "            current_potentials = input_potentials.copy()\n",
    "\n",
    "            prediction_mask = predictions.astype(np.float64)\n",
    "            for example_idx in range(self.test_set_size):\n",
    "                domain_idx = int(self.gt_domain[example_idx])\n",
    "                if domain_idx == NO_DOMAIN:\n",
    "                    continue # This example has no domain present, it can't affect the constraints.\n",
    "                current_potentials[example_idx, :] -= prediction_mask[example_idx, :] * lambdas[:, 0] * constraints[:, 0, domain_idx]\n",
    "                current_potentials[example_idx, :] -= prediction_mask[example_idx, :] * lambdas[:, 1] * constraints[:, 1, domain_idx]\n",
    "\n",
    "            if (epoch % 10 == 0 or epoch == self.total_epochs-1) and self.verbosity >= 2:\n",
    "                print('Finished %i-th Epoch.' % epoch)\n",
    "                mean_bias = np.mean(np.abs(0.5 - self.compute_bias(current_potentials)))\n",
    "                print('\\tMean Bias: %0.4f' % mean_bias)\n",
    "                constraint_count = len(constraints)\n",
    "                print('\\tConstraint Satisfaction: %i/%i' % (constraint_count-violated_constraint_count, constraint_count))\n",
    "                current_mAP = self.compute_mAP(current_potentials)\n",
    "                current_class_acc = self.compute_accuracy(current_potentials)\n",
    "                total_flipped_predictions = np.count_nonzero(self.multiclass_inference(current_potentials) != initial_predictions)\n",
    "                print('\\tTotal Flipped Predictions: %i' % total_flipped_predictions)\n",
    "                print('\\tCurrent mAP: %0.2f%%' % current_mAP)\n",
    "                print('\\tCurrent Class Acc: %0.2f%%' % current_class_acc)\n",
    "\n",
    "            if violated_constraint_count == 0:\n",
    "                break\n",
    "        if self.verbosity >= 1:\n",
    "            final_mAP = self.compute_mAP(current_potentials)\n",
    "            final_bias = np.mean(np.abs(0.5 - self.compute_bias(current_potentials)))\n",
    "            final_bias_amplification = self.compute_bias_amplification(current_potentials)\n",
    "            name_in = ('%s, after optimization' % self.method_name).ljust(85)\n",
    "            print('%s mAP. %0.2f%%. Bias %0.3f' % (name_in, final_mAP, final_bias))\n",
    "            print('\\t bias amplification: %0.4f. ' % final_bias_amplification)\n",
    "            print('mAP change %f, bias change %f, bias amplication change %f' \n",
    "                  % (final_mAP-initial_mAP, final_bias-initial_bias, final_bias_amplification-initial_bias_amplification))\n",
    "        return current_potentials, self.multiclass_inference(current_potentials)\n",
    "\n",
    "def compute_thresh_on_dev(dev_potentials, dev_targets, reduction_method='sum'):\n",
    "    thresholds = [0.1 * (i+1) for i in range(9)]\n",
    "        \n",
    "    class_count = dev_potentials.shape[1] // 2\n",
    "    output_threshes = np.zeros((class_count,), dtype=np.float64)\n",
    "    dev_potentials = sigmoid(dev_potentials)\n",
    "        \n",
    "    if reduction_method == 'sum':\n",
    "        dev_potentials = (dev_potentials[:, :class_count] + dev_potentials[:, class_count:]) / 2.0  \n",
    "    elif reduction_method == 'condition':\n",
    "        gt_domain = compute_gt_domain_from_labels(dev_targets)\n",
    "        selected_outputs = []\n",
    "        for i in range(dev_potentials.shape[0]):\n",
    "            cur_domain = gt_domain[i]\n",
    "            if cur_domain == MALE:\n",
    "                selected_outputs.append(dev_potentials[i, :class_count])\n",
    "            elif cur_domain == FEMALE:\n",
    "                selected_outputs.append(dev_potentials[i, class_count:])\n",
    "            elif cur_domain == NO_DOMAIN:\n",
    "                # We can't condition because there is no domain to condition on. So just average.\n",
    "                current_outputs = dev_potentials[i, :class_count] + dev_potentials[i, class_count:]\n",
    "                selected_outputs.append(current_outputs / 2.0)\n",
    "        dev_potentials = np.stack(selected_outputs, axis=0)\n",
    "\n",
    "    dev_targets = dev_targets.astype(np.bool) \n",
    "    dev_targets = dev_targets[:, :class_count] | dev_targets[:, class_count:]\n",
    "    probs = dev_potentials\n",
    "    for ci in range(class_count):\n",
    "        output_threshes[ci] = max_fscore(dev_targets[:, ci], probs[:, ci])[1]\n",
    "    return output_threshes\n",
    "\n",
    "def max_fscore(targets,scores):\n",
    "    # sorting the scores and the targets\n",
    "    ssn = zip(scores,range(len(scores)))\n",
    "    ssn = sorted(ssn,reverse=True)\n",
    "    ts = [targets[ssn[i][1]] for i in range(len(ssn))]\n",
    "\n",
    "    num_pos = np.sum(ts)\n",
    "    true_pos = np.cumsum(ts)\n",
    "    # f-score at each threshold\n",
    "    # 2/(1/recall+1/precision) = 2/(npos/tp+n/tp)=2*tp/(npos+n)\n",
    "    f = 2*true_pos/(num_pos + range(len(ts))+1)\n",
    "    ii=np.argmax(f)\n",
    "\n",
    "    ff = f[ii]\n",
    "    thr = ssn[ii][0]\n",
    "    fcheck = f_score(targets,scores,thr)\n",
    "    # check:\n",
    "    if not ff == fcheck:\n",
    "        print('Check failed')\n",
    "        assert(False)\n",
    "    # return the f-score and the corresponding threshold (>= )\n",
    "    return ff,thr\n",
    "\n",
    "def f_score(targets,scores,thr):\n",
    "    num_pos = np.sum(targets)\n",
    "    true_pos = np.sum(np.logical_and(scores >= thr,targets == 1))\n",
    "    num_pred = np.sum([scores >= thr])\n",
    "    return 2*true_pos/(num_pos+num_pred)\n",
    "\n",
    "def compute_gt_domain_from_labels(labels):\n",
    "    test_set_size, twice_class_count = labels.shape\n",
    "    class_count = twice_class_count // 2\n",
    "    gt_domain = np.zeros((test_set_size,), dtype=np.float64)\n",
    "    for i in range(test_set_size):\n",
    "        has_male_output = np.any(labels[i, :class_count])\n",
    "        has_female_output = np.any(labels[i, class_count:])\n",
    "        if has_male_output and not has_female_output:\n",
    "            gt_domain[i] = MALE\n",
    "        if has_female_output and not has_male_output:\n",
    "            gt_domain[i] = FEMALE\n",
    "        if not has_female_output and not has_male_output:\n",
    "            gt_domain[i] = NO_DOMAIN\n",
    "        assert not (has_male_output and has_female_output)\n",
    "    return gt_domain\n",
    "\n",
    "def run(hparams, data):\n",
    "    optimize_probabilities = hparams['optimize_probabilities']\n",
    "    reduction_method = hparams['reduction']\n",
    "    apply_prior_shift = hparams['prior_shift']\n",
    "\n",
    "    expected_test_set_size = data['targets'].shape[0]\n",
    "    expected_class_count = data['targets'].shape[1] // 2\n",
    "    gt_labels = data['targets'].astype(np.int32)\n",
    "    gt_domain = compute_gt_domain_from_labels(gt_labels)\n",
    "    twon_activations = data['outputs']\n",
    "    \n",
    "    inference_thresholds = compute_thresh_on_dev(data['dev_outputs'], data['dev_targets'],\n",
    "                                                     reduction_method=reduction_method)\n",
    "\n",
    "    gender_count = np.sum(gt_labels, axis=0)\n",
    "    target_domain_ratios = gender_count[:expected_class_count] / (\n",
    "        gender_count[:expected_class_count] + gender_count[expected_class_count:])\n",
    "\n",
    "    domain_labels = ['Male', 'Female']\n",
    "    train_targets = data['train_targets']\n",
    "        \n",
    "    selected_outputs = []\n",
    "    twon_activations = sigmoid(twon_activations)\n",
    "        \n",
    "    if reduction_method == 'sum':\n",
    "        outputs = (twon_activations[:, :expected_class_count] + twon_activations[:, expected_class_count:]) / 2.0\n",
    "\n",
    "    if reduction_method == 'condition':\n",
    "        for i in range(twon_activations.shape[0]):\n",
    "            cur_domain = gt_domain[i]\n",
    "            if cur_domain == MALE:\n",
    "                selected_outputs.append(twon_activations[i, :expected_class_count])\n",
    "            elif cur_domain == FEMALE:\n",
    "                selected_outputs.append(twon_activations[i, expected_class_count:])\n",
    "            elif cur_domain == NO_DOMAIN:\n",
    "                current_outputs = (twon_activations[i, :expected_class_count] +\n",
    "                                   twon_activations[i, expected_class_count:])\n",
    "                selected_outputs.append(current_outputs / 2.0)\n",
    "            else:\n",
    "                assert False\n",
    "        outputs = np.stack(selected_outputs, axis=0)\n",
    "\n",
    "    assert outputs.shape == (expected_test_set_size, expected_class_count)\n",
    "    assert gt_domain.shape == (expected_test_set_size,)\n",
    "    margin = 0.05\n",
    "    lr = hparams['lr']\n",
    "    input_potentials = outputs\n",
    "    \n",
    "    optimization_str = 'optimize on probabilities' if hparams['optimize_probabilities'] else 'optimize on outputs'\n",
    "    if hparams['optimize_probabilities'] and hparams['reduction'] == 'sum':\n",
    "        reduction_str = 'sum probabilities'\n",
    "    elif not hparams['optimize_probabilities'] and hparams['reduction'] =='sum':\n",
    "        reduction_str = 'sum outputs'\n",
    "    elif not hparams['optimize_probabilities'] and hparams['reduction'] == 'condition':\n",
    "        reduction_str = 'condition on d0'\n",
    "    else:\n",
    "        assert False\n",
    "    prior_shift_str = 'prior shift' if hparams['prior_shift'] else 'no prior shift'\n",
    "    method_str = '%s, %s, %s' % (reduction_str, optimization_str, prior_shift_str)\n",
    "    \n",
    "    optimizer = optimize_potentials_given_known_domain(gt_labels=gt_labels,\n",
    "                                                       gt_domain=gt_domain,\n",
    "                                                       lr=lr,\n",
    "                                                       margin=margin,\n",
    "                                                       apply_prior_shift=apply_prior_shift,\n",
    "                                                       inputs_are_activations=(not optimize_probabilities),\n",
    "                                                       method_name=method_str,\n",
    "                                                       target_domain_ratios=target_domain_ratios,\n",
    "                                                       domain_labels=domain_labels,\n",
    "                                                       inference_thresholds=inference_thresholds,\n",
    "                                                       training_set_targets=train_targets,\n",
    "                                                       verbosity=hparams['verbosity'],\n",
    "                                                       total_epochs=hparams['total_epochs'])\n",
    "    \n",
    "    output_potentials, output_classes = optimizer.optimize(input_potentials)\n",
    "    return output_potentials, output_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/celeba/labels_dict', 'rb') as f:\n",
    "    celeba_labels_dict = pickle.load(f)\n",
    "with open('../data/celeba/train_key_list', 'rb') as f:\n",
    "    train_key_list = pickle.load(f)\n",
    "with open('../data/celeba/dev_key_list', 'rb') as f:\n",
    "    dev_key_list = pickle.load(f)\n",
    "with open('../data/celeba/test_key_list', 'rb') as f:\n",
    "    test_key_list = pickle.load(f)\n",
    "with open('../data/celeba/subclass_idx', 'rb') as f:\n",
    "    subclass_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_array = np.array([celeba_labels_dict[key] for key in train_key_list])\n",
    "dev_target_array = np.array([celeba_labels_dict[key] for key in dev_key_list])\n",
    "test_target_array = np.array([celeba_labels_dict[key] for key in test_key_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.hstack((train_target_array[:, subclass_idx]*train_target_array[:, -1:], \n",
    "                           train_target_array[:, subclass_idx]*(1-train_target_array[:, -1:])))\n",
    "dev_targets = np.hstack((dev_target_array[:, subclass_idx] * dev_target_array[:, -1:], \n",
    "                         dev_target_array[:, subclass_idx] * (1-dev_target_array[:, -1:])))\n",
    "test_targets = np.hstack((test_target_array[:, subclass_idx] * test_target_array[:, -1:], \n",
    "                          test_target_array[:, subclass_idx] * (1-test_target_array[:, -1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to corresponding result path\n",
    "dev_result_path = '../record/celeba_domain_discriminative/celeba_domain_discriminative_e1/dev_result.pkl'\n",
    "test_result_path = '../record/celeba_domain_discriminative/celeba_domain_discriminative_e1/test_result.pkl'\n",
    "\n",
    "with open(dev_result_path, 'rb') as f:\n",
    "    dev = pickle.load(f)\n",
    "with open(test_result_path, 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_outputs = dev['output'][:, subclass_idx + [item+39 for item in subclass_idx]]\n",
    "test_outputs = test['output'][:, subclass_idx + [item+39 for item in subclass_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'targets': test_targets, 'outputs': test_outputs, \n",
    "        'dev_targets': dev_targets, 'dev_outputs': dev_outputs,\n",
    "        'train_targets': train_targets,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'optimize_probabilities':True, 'reduction':'sum', 'prior_shift':False, \n",
    "           'lr': 1e-5, 'total_epochs': 300, 'verbosity': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = run(hparams, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
