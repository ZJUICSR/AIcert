{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bcccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from models.resnet import *\n",
    "from models.vggnet import *\n",
    "from models.mynet import *\n",
    "from logic_units import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f575bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronsActivate:\n",
    "    def __init__(self, model,data,threshold):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.threshold = threshold\n",
    "    def get_neurons_activate(self,target_layer):\n",
    "        sample_num=self.data.shape[0] #样本个数\n",
    "        neurons_activate_dict=torch.zeros(sample_num,1).to(device)\n",
    "        layer_dict = self.get_model_layers()\n",
    "        for layer, module in layer_dict.items():\n",
    "            #print('ceng:',layer,module)\n",
    "            if layer==target_layer:\n",
    "                outputs = torch.squeeze(self.extract_outputs(module))\n",
    "                scaled_outputs = self.scale(outputs)\n",
    "                sample_layer_outputs=scaled_outputs.view(sample_num,-1)  #sample_layer_outputs表示所有样本的某层输出--神经元激活值\n",
    "                activation=torch.gt(sample_layer_outputs, self.threshold)  #大于门限则激活\n",
    "                neurons_activate_dict=torch.cat([neurons_activate_dict, activation], dim=1)\n",
    "        return neurons_activate_dict.detach().cpu().numpy()\n",
    "    def step_through_model(self, model,prefix=''):\n",
    "        for name, module in model.named_children():\n",
    "            path = '{}/{}'.format(prefix, name)\n",
    "            if (isinstance(module, nn.Conv1d)\n",
    "                or isinstance(module, nn.Conv2d)\n",
    "                or isinstance(module, nn.Linear)): # test for dataset\n",
    "                yield (path, name, module)\n",
    "            else:\n",
    "                yield from self.step_through_model(module, path)\n",
    "    def get_model_layers(self, cross_section_size=0):\n",
    "        layer_dict = {}\n",
    "        i = 0\n",
    "        for (path, name, module) in self.step_through_model(self.model):\n",
    "            layer_dict[str(i) + path] = module\n",
    "            i += 1\n",
    "        if cross_section_size > 0:\n",
    "            target_layers = list(layer_dict)[0::cross_section_size] \n",
    "            layer_dict = { target_layer: layer_dict[target_layer] for target_layer in target_layers }\n",
    "        return layer_dict\n",
    "\n",
    "    def scale(self, out, rmax=1, rmin=0):\n",
    "        output_std = (out - out.min()) / (out.max() - out.min())\n",
    "        output_scaled = output_std * (rmax - rmin) + rmin\n",
    "        return output_scaled\n",
    "\n",
    "    def extract_outputs(self,module, force_relu=True):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            if force_relu:\n",
    "                outputs.append(torch.relu(output))   \n",
    "            else:\n",
    "                outputs.append(output)\n",
    "        handle = module.register_forward_hook(hook)     \n",
    "        self.model(self.data)\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96818eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DSC\n",
    "class DSC(object):\n",
    "    def __init__(self, train, label, model, layers, u=2, k_bins=1000, threshold=10 ** -5):\n",
    "        '''\n",
    "        train:训练集数据\n",
    "        label:训练集的标签\n",
    "        model:输入模型\n",
    "        layers:输出张量层\n",
    "        std : 方差筛选\n",
    "        u : 上界\n",
    "        k_bins: 分割段数\n",
    "        threshold: 阈值筛选\n",
    "        '''\n",
    "        self.train = train\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.lst = []\n",
    "        self.std_lst = []\n",
    "        self.mask = []\n",
    "        self.neuron_activate_train = []\n",
    "        index_lst = []\n",
    "        self.u = u\n",
    "        self.k_bins = k_bins\n",
    "        self.threshold = threshold\n",
    "        self.test_score = []\n",
    "\n",
    "        batch_size=128\n",
    "        datalist=torch.split(train, batch_size, dim=0)\n",
    "        neurons_activate=[]\n",
    "        for data_batch in datalist:         \n",
    "            na=NeuronsActivate(model,data_batch,0.0)\n",
    "            batch_neurons_activate=na.get_neurons_activate(self.layers)\n",
    "            #upper = (batch_neurons_activate > 0.0)  # nc准则：大于门限就是激活\n",
    "            #batch_coverage = np.sum(upper,axis=1)  # 统计激活了的神经元的个数\n",
    "            neurons_activate.append(batch_neurons_activate)\n",
    "        \n",
    "        self.neuron_activate_train = np.concatenate(neurons_activate, axis=0)\n",
    "        print('neuron_train',self.neuron_activate_train.shape)\n",
    "        self.train_label = label.cpu().detach().numpy()\n",
    "        self.lst = list(zip(index_lst, self.lst))\n",
    "\n",
    "    def find_closest_at(self, at, train_ats):\n",
    "        dist = np.linalg.norm(at - train_ats, axis=1)  # 二范数值\n",
    "        return (min(dist), train_ats[np.argmin(dist)])  # 找到二范数值最近的,同时把结果返回去\n",
    "\n",
    "    def fit(self, test, label):\n",
    "        time_limit = 43200\n",
    "        start = time.time()\n",
    "        self.neuron_activate_test = []\n",
    "        \n",
    "        batch_size=128\n",
    "        datalist=torch.split(test, batch_size, dim=0)\n",
    "        neurons_activate=[]\n",
    "        for data_batch in datalist:         \n",
    "            na=NeuronsActivate(model,data_batch,0.0)\n",
    "            batch_neurons_activate=na.get_neurons_activate(self.layers)\n",
    "            #upper = (batch_neurons_activate > 0.0)  # nc准则：大于门限就是激活\n",
    "            #batch_coverage = np.sum(upper,axis=1)  # 统计激活了的神经元的个数\n",
    "            neurons_activate.append(batch_neurons_activate)\n",
    "        \n",
    "        self.neuron_activate_test = np.concatenate(neurons_activate, axis=0)\n",
    "\n",
    "        class_matrix = {}\n",
    "        all_idx = []\n",
    "        for i, lb in enumerate(self.train_label):\n",
    "            if lb not in class_matrix:\n",
    "                class_matrix[lb] = []\n",
    "            class_matrix[lb].append(i)\n",
    "            all_idx.append(i)\n",
    "        # print(class_matrix)\n",
    "\n",
    "        # time_limit = 10\n",
    "\n",
    "        # dsa代码  这里也写错了,我们的代码没有找新的参考点,而是还是用的测试集\n",
    "        for test_sample, label_sample in tqdm(zip(self.neuron_activate_test, label)):\n",
    "            end = time.time()\n",
    "            if end - start >= time_limit:\n",
    "                print(\"=======================time limit=======================\")\n",
    "                return None\n",
    "            # print(\"剩余时间: {}\".format(time_limit - (end - start)))\n",
    "            x = self.neuron_activate_train[class_matrix[label_sample]]\n",
    "            a_dist, a_dot = self.find_closest_at(test_sample, x)\n",
    "            y = self.neuron_activate_train[list(set(all_idx) - set(class_matrix[label_sample]))]\n",
    "            b_dist, _ = self.find_closest_at(\n",
    "                a_dot, y\n",
    "            )  # 求出最近的距离值\n",
    "            self.test_score.append(a_dist / b_dist)\n",
    "\n",
    "        # for test_sample, label_sample in tqdm(zip(self.neuron_activate_test, label)):\n",
    "        #     dist_a = np.min(\n",
    "        #         ((self.neuron_activate_train[self.train_label ==   label_sample, :] - test_sample) ** 2).sum(axis=1))\n",
    "        #     dist_b = np.min(\n",
    "        #         ((self.neuron_activate_train[self.train_label != label_sample, :] - test_sample) ** 2).sum(axis=1))\n",
    "        #     self.test_score.append(dist_a / dist_b)\n",
    "        bins = np.linspace(np.amin(self.test_score), self.u, self.k_bins)\n",
    "        x = np.unique(np.digitize(self.test_score, bins))\n",
    "        rate = len(np.unique(x)) / float(self.k_bins)\n",
    "        return rate\n",
    "\n",
    "    def get_sore(self):\n",
    "        return self.test_score\n",
    "\n",
    "    # def get_rate(self, u=None, k_bins=None, auto=False):\n",
    "    #     if auto:\n",
    "    #         self.u = np.max(np.array(self.test_score))  # 将u中最大值设置为上界\n",
    "    #     else:\n",
    "    #         self.u = u\n",
    "    #     bins = np.linspace(np.amin(self.test_score), self.u, self.k_bins)\n",
    "    #     x = np.unique(np.digitize(self.test_score, bins))\n",
    "    #     rate = len(np.unique(x)) / float(k_bins)\n",
    "    #     return rate\n",
    "\n",
    "    def get_u(self):\n",
    "        return self.u\n",
    "\n",
    "    def rank_2(self):\n",
    "        return np.argsort(self.get_sore())[::-1]  # 由大到小排序\n",
    "\n",
    "    def rank_fast(self):\n",
    "        bins = np.linspace(np.amin(self.test_score), self.u, self.k_bins)\n",
    "        score_bin = np.digitize(self.test_score, bins)\n",
    "        score_bin_uni = np.unique(score_bin)\n",
    "        res_idx_arr = []\n",
    "        for x in score_bin_uni:\n",
    "            np.random.seed(41)\n",
    "            idx_arr = np.argwhere(score_bin == x).flatten()\n",
    "            idx = np.random.choice(idx_arr)\n",
    "            res_idx_arr.append(idx)\n",
    "        return res_idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b4ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_level(pred_test_prob,true_test):#新增评价指标：严重性指标     \n",
    "    error_level=[]\n",
    "    pred_test_sort=np.argsort(-pred_test_prob, axis=1)\n",
    "    for i in range(len(pred_test_prob)):\n",
    "        if pred_test_sort[i][0]==true_test[i]:\n",
    "            error_level.append(0)\n",
    "        elif pred_test_sort[i][1]==true_test[i]:\n",
    "            error_level.append(5)\n",
    "        elif pred_test_sort[i][2]==true_test[i]:\n",
    "            error_level.append(10)\n",
    "        else:\n",
    "            error_level.append(100)\n",
    "    return error_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f274ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliang/anaconda3/envs/logic/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsc..\n",
      "neuron_train (3000, 2049)\n",
      "dsc..  fit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16640it [07:41, 36.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model = vgg16_bn().to(device)\n",
    "model_name='vgg16'\n",
    "dataset_name='cifar'            \n",
    "\n",
    "model.load_state_dict(torch.load('../adv_train/model-vgg16-cifar10/Standard-cifar10-model-vgg16-epoch300.pt'))\n",
    "model.eval()\n",
    "images=torch.load('images_of_TestCaseSet_vgg16_cifar10.pt')\n",
    "labels=torch.load('labels_of_TestCaseSet_vgg16_cifar10.pt')\n",
    "data=images\n",
    "true_test=labels\n",
    "\n",
    "coverage=\"DSC\"\n",
    "index=-1\n",
    "\n",
    "# 初始化变量\n",
    "k_bins = 1000\n",
    "\n",
    "# 计算覆盖\n",
    "layers='12/features/40'\n",
    "\n",
    "batch_size=128\n",
    "datalist=torch.split(data, batch_size, dim=0)\n",
    "\n",
    "pred_test_prob=[]\n",
    "for data_batch in datalist:\n",
    "    output=model(data_batch.to(device))\n",
    "    prob = F.softmax(output)\n",
    "    pred_one=prob.cpu().detach()\n",
    "    pred_test_prob.append(pred_one)\n",
    "pred_test_prob=torch.cat(pred_test_prob,dim=0)\n",
    "pred_test_prob=pred_test_prob.numpy()\n",
    "pred_test=np.argmax(pred_test_prob, axis=1)\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root='../dataset/data', train=False,download=False, transform=transforms.ToTensor())\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=3000, shuffle=True)\n",
    "image_iter = iter(val_dataloader)\n",
    "train, Y_train = image_iter.next()\n",
    "train, Y_train=train.to(device),Y_train.to(device)\n",
    "\n",
    "# print(\"test len\")\n",
    "# print(len(test))\n",
    "metrics = None\n",
    "rate = None\n",
    "rank_lst_time = None\n",
    "rank_lst2_time = None\n",
    "st = time.time()  # 计算排序时间\n",
    "if coverage == \"DSC\":\n",
    "    print(\"dsc..\")\n",
    "    metrics = DSC(train, Y_train, model, layers, k_bins=k_bins)\n",
    "    print(\"dsc..  fit...\")\n",
    "    rate = metrics.fit(data, pred_test)\n",
    "    #  model.get_rate()  # 获得覆盖率\n",
    "en = time.time()\n",
    "pre_time = st - en\n",
    "start = time.time()\n",
    "rank_lst2 = metrics.rank_2()\n",
    "end = time.time()\n",
    "rank_lst2_time = start - end + pre_time\n",
    "start = time.time()\n",
    "rank_lst = metrics.rank_fast()\n",
    "end = time.time()\n",
    "rank_lst_time = end - start + pre_time\n",
    "# score = model.get_sore()  # 获得分数\n",
    "u = metrics.get_u()  # 获得上界\n",
    "# 构造结果\n",
    "df = pd.DataFrame([])\n",
    "# df[\"LSA\"] = score\n",
    "# print(pred_test)\n",
    "\n",
    "true_test=true_test.cpu().numpy()\n",
    "\n",
    "df['right'] = (pred_test == true_test).astype('int')  # right\n",
    "df['cam'] = 0\n",
    "df['cam'].loc[rank_lst] = list(range(1, len(rank_lst) + 1))  # cam\n",
    "df['ctm'] = 0\n",
    "df['ctm'].loc[rank_lst2] = list(range(1, len(rank_lst2) + 1))  # ctm\n",
    "df['rate'] = rate  # tate\n",
    "df['cam_time'] = rank_lst_time\n",
    "df['ctm_time'] = rank_lst2_time\n",
    "\n",
    "df['error_level']=error_level(pred_test_prob,true_test)\n",
    "\n",
    "if rate is None:\n",
    "    df[\"overtime\"] = 1\n",
    "# 数据集_覆盖方法_分箱_上界_选择的层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d04370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./all_output/output_cifar/{}/{}_dsc_k_{}_u_{}.csv'.format('vgg16', 'cifar', k_bins, u))\n",
    "df.to_csv('./all_output/output_cifar/{}/{}_dsc_0.csv'.format('vgg16', 'cifar',0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007be6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91f310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
