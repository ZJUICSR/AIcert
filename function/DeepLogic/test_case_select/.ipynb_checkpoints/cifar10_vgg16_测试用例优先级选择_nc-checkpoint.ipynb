{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8ee1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0f21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from imagenet_labels import *\n",
    "from models.resnet import *\n",
    "from models.vggnet import *\n",
    "from models.mynet import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#train_dataset = datasets.CIFAR10(root='./data', train=True,download=False, transform=data_transform)\n",
    "val_dataset = datasets.CIFAR10(root='../dataset/data', train=False,download=False, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model = vgg16_bn().to(device)\n",
    "model.load_state_dict((torch.load('../adv_train/model-vgg16-cifar10/Standard-cifar10-model-vgg16-epoch300.pt')))#评估普通模型-干净样本准确率\n",
    "model = model.to(device).eval()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "    'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#加载测试数据集\n",
    "images=torch.load('images_of_TestCaseSet_vgg16_cifar10.pt')\n",
    "labels=torch.load('labels_of_TestCaseSet_vgg16_cifar10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e30b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16640])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7974dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronsActivate:\n",
    "    def __init__(self, model,data,threshold):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.threshold = threshold\n",
    "    def get_neurons_activate(self):\n",
    "        sample_num=self.data.shape[0] #样本个数\n",
    "        neurons_activate_dict=torch.zeros(sample_num,1).to(device)\n",
    "        layer_dict = self.get_model_layers()\n",
    "        for layer, module in layer_dict.items():\n",
    "            outputs = torch.squeeze(self.extract_outputs(module))\n",
    "            scaled_outputs = self.scale(outputs)\n",
    "            sample_layer_outputs=scaled_outputs.view(sample_num,-1)  #sample_layer_outputs表示所有样本的某层输出--神经元激活值\n",
    "            activation=torch.gt(sample_layer_outputs, self.threshold)  #大于门限则激活\n",
    "            neurons_activate_dict=torch.cat([neurons_activate_dict, activation], dim=1)\n",
    "        return neurons_activate_dict.detach().cpu().numpy()\n",
    "    def step_through_model(self, model,prefix=''):\n",
    "        for name, module in model.named_children():\n",
    "            path = '{}/{}'.format(prefix, name)\n",
    "            if (isinstance(module, nn.Conv1d)\n",
    "                or isinstance(module, nn.Conv2d)\n",
    "                or isinstance(module, nn.Linear)): # test for dataset\n",
    "                yield (path, name, module)\n",
    "            else:\n",
    "                yield from self.step_through_model(module, path)\n",
    "    def get_model_layers(self, cross_section_size=0):\n",
    "        layer_dict = {}\n",
    "        i = 0\n",
    "        for (path, name, module) in self.step_through_model(self.model):\n",
    "            layer_dict[str(i) + path] = module\n",
    "            i += 1\n",
    "        if cross_section_size > 0:\n",
    "            target_layers = list(layer_dict)[0::cross_section_size] \n",
    "            layer_dict = { target_layer: layer_dict[target_layer] for target_layer in target_layers }\n",
    "        return layer_dict\n",
    "\n",
    "    def scale(self, out, rmax=1, rmin=0):\n",
    "        output_std = (out - out.min()) / (out.max() - out.min())\n",
    "        output_scaled = output_std * (rmax - rmin) + rmin\n",
    "        return output_scaled\n",
    "\n",
    "    def extract_outputs(self,module, force_relu=True):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            if force_relu:\n",
    "                outputs.append(torch.relu(output))   \n",
    "            else:\n",
    "                outputs.append(output)\n",
    "        handle = module.register_forward_hook(hook)     \n",
    "        self.model(self.data)\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b71e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=images[:1024]\n",
    "true_test=labels[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e42b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.31800442934036255 88251 277515\n",
      "1 0.46486857533454895 129008 277515\n",
      "2 0.5345441102981567 148344 277515\n",
      "3 0.5891609787940979 163501 277515\n",
      "4 0.6477668285369873 179765 277515\n",
      "5 0.6906005144119263 191652 277515\n",
      "6 0.7271787524223328 201803 277515\n",
      "7 0.7482910752296448 207662 277515\n",
      "8 0.7650073170661926 212301 277515\n",
      "9 0.7806605100631714 216645 277515\n",
      "10 0.7930166125297546 220074 277515\n",
      "11 0.8068861365318298 223923 277515\n",
      "12 0.8204025030136108 227674 277515\n",
      "13 0.8337783813476562 231386 277515\n",
      "14 0.8419508934020996 233654 277515\n",
      "15 0.8494243621826172 235728 277515\n",
      "16 0.8587644100189209 238320 277515\n",
      "17 0.8647244572639465 239974 277515\n",
      "18 0.868911623954773 241136 277515\n",
      "19 0.873855471611023 242508 277515\n",
      "20 0.8779705762863159 243650 277515\n",
      "21 0.8807992339134216 244435 277515\n",
      "22 0.8835198283195496 245190 277515\n",
      "23 0.8889105319976807 246686 277515\n",
      "24 0.891677975654602 247454 277515\n",
      "25 0.8943480849266052 248195 277515\n",
      "26 0.8971623182296753 248976 277515\n",
      "27 0.8994180560112 249602 277515\n",
      "28 0.9014900326728821 250177 277515\n",
      "29 0.9041925668716431 250927 277515\n",
      "30 0.9063654541969299 251530 277515\n",
      "31 0.9076986908912659 251900 277515\n",
      "32 0.9093202352523804 252350 277515\n",
      "33 0.9106282591819763 252713 277515\n",
      "34 0.911730945110321 253019 277515\n",
      "35 0.9133596420288086 253471 277515\n",
      "36 0.9146496653556824 253829 277515\n",
      "37 0.9158352017402649 254158 277515\n",
      "38 0.9169414639472961 254465 277515\n",
      "39 0.9180513024330139 254773 277515\n",
      "40 0.919326901435852 255127 277515\n",
      "41 0.9204763770103455 255446 277515\n",
      "42 0.921661913394928 255775 277515\n",
      "43 0.9222925305366516 255950 277515\n",
      "44 0.9232690334320068 256221 277515\n",
      "45 0.923928439617157 256404 277515\n",
      "46 0.9246779680252075 256612 277515\n",
      "47 0.9256256818771362 256875 277515\n",
      "48 0.9260761141777039 257000 277515\n",
      "49 0.9263787865638733 257084 277515\n",
      "50 0.9270706176757812 257276 277515\n",
      "51 0.9275751113891602 257416 277515\n",
      "52 0.9281228184700012 257568 277515\n",
      "53 0.9289444088935852 257796 277515\n",
      "54 0.9293839931488037 257918 277515\n",
      "55 0.929618239402771 257983 277515\n",
      "56 0.9306163787841797 258260 277515\n",
      "57 0.9352395534515381 259543 277515\n",
      "58 0.9356935620307922 259669 277515\n",
      "59 0.9362521171569824 259824 277515\n",
      "60 0.9365260004997253 259900 277515\n",
      "61 0.9370844960212708 260055 277515\n",
      "62 0.9380285739898682 260317 277515\n",
      "63 0.9384537935256958 260435 277515\n",
      "64 0.9388033151626587 260532 277515\n",
      "65 0.9392717480659485 260662 277515\n",
      "66 0.9396933913230896 260779 277515\n",
      "67 0.940201461315155 260920 277515\n",
      "68 0.9407347440719604 261068 277515\n",
      "69 0.9410662651062012 261160 277515\n",
      "70 0.9413005113601685 261225 277515\n",
      "71 0.9415887594223022 261305 277515\n",
      "72 0.9417184591293335 261341 277515\n",
      "73 0.9419419169425964 261403 277515\n",
      "74 0.9422121644020081 261478 277515\n",
      "75 0.9425652623176575 261576 277515\n",
      "76 0.9428463578224182 261654 277515\n",
      "77 0.9431886672973633 261749 277515\n",
      "78 0.9436823725700378 261886 277515\n",
      "79 0.9438264966011047 261926 277515\n",
      "80 0.9439994692802429 261974 277515\n",
      "81 0.9461326599121094 262566 277515\n",
      "82 0.9462732076644897 262605 277515\n",
      "83 0.9465182423591614 262673 277515\n",
      "84 0.9466371536254883 262706 277515\n",
      "85 0.9467416405677795 262735 277515\n",
      "86 0.9471812844276428 262857 277515\n",
      "87 0.9472677111625671 262881 277515\n",
      "88 0.9474154710769653 262922 277515\n",
      "89 0.9476425051689148 262985 277515\n",
      "90 0.9477866291999817 263025 277515\n",
      "91 0.947952389717102 263071 277515\n",
      "92 0.9481397867202759 263123 277515\n",
      "93 0.9482586979866028 263156 277515\n",
      "94 0.9483739733695984 263188 277515\n",
      "95 0.948748767375946 263292 277515\n",
      "96 0.9491559267044067 263405 277515\n",
      "97 0.9492459893226624 263430 277515\n",
      "98 0.9494298100471497 263481 277515\n",
      "99 0.9496135711669922 263532 277515\n",
      "100 0.9497612714767456 263573 277515\n",
      "101 0.949901819229126 263612 277515\n",
      "102 0.9510585069656372 263933 277515\n",
      "103 0.9514333009719849 264037 277515\n",
      "104 0.9516098499298096 264086 277515\n",
      "105 0.9518080353736877 264141 277515\n",
      "106 0.9519630074501038 264184 277515\n",
      "107 0.9521071314811707 264224 277515\n",
      "108 0.9523665904998779 264296 277515\n",
      "109 0.9525899887084961 264358 277515\n",
      "110 0.9527413249015808 264400 277515\n",
      "111 0.9531629085540771 264517 277515\n",
      "112 0.953220546245575 264533 277515\n",
      "113 0.9532962441444397 264554 277515\n",
      "114 0.9534547924995422 264598 277515\n",
      "115 0.9535989165306091 264638 277515\n",
      "116 0.9537070393562317 264668 277515\n",
      "117 0.9538403749465942 264705 277515\n",
      "118 0.956344723701477 265400 277515\n",
      "119 0.9567050933837891 265500 277515\n",
      "120 0.9567447304725647 265511 277515\n",
      "121 0.9568852186203003 265550 277515\n",
      "122 0.9570618271827698 265599 277515\n",
      "123 0.9571626782417297 265627 277515\n",
      "124 0.9571987390518188 265637 277515\n",
      "125 0.9573392868041992 265676 277515\n",
      "126 0.9574365615844727 265703 277515\n",
      "127 0.9576419591903687 265760 277515\n",
      "128 0.9577356576919556 265786 277515\n",
      "129 0.957832932472229 265813 277515\n",
      "130 0.9578942060470581 265830 277515\n",
      "131 0.9580707550048828 265879 277515\n",
      "132 0.9581428170204163 265899 277515\n",
      "133 0.9583950638771057 265969 277515\n",
      "134 0.9584815502166748 265993 277515\n",
      "135 0.9585752487182617 266019 277515\n",
      "136 0.9587445855140686 266066 277515\n",
      "137 0.9589536190032959 266124 277515\n",
      "138 0.9589860439300537 266133 277515\n",
      "139 0.9590725302696228 266157 277515\n",
      "140 0.9591373801231384 266175 277515\n",
      "141 0.9592454433441162 266205 277515\n",
      "142 0.9593355655670166 266230 277515\n",
      "143 0.9595553874969482 266291 277515\n",
      "144 0.959613025188446 266307 277515\n",
      "145 0.9596958756446838 266330 277515\n",
      "146 0.9597535729408264 266346 277515\n",
      "147 0.959843635559082 266371 277515\n",
      "148 0.9600778818130493 266436 277515\n",
      "149 0.9602147936820984 266474 277515\n",
      "150 0.9604237675666809 266532 277515\n",
      "151 0.9605643153190613 266571 277515\n",
      "152 0.9606183767318726 266586 277515\n",
      "153 0.9607336521148682 266618 277515\n",
      "154 0.9607769250869751 266630 277515\n",
      "155 0.9608273506164551 266644 277515\n",
      "156 0.9608597755432129 266653 277515\n",
      "157 0.9609426856040955 266676 277515\n",
      "158 0.9610327482223511 266701 277515\n",
      "159 0.9610723853111267 266712 277515\n",
      "160 0.9610832333564758 266715 277515\n",
      "161 0.9611228704452515 266726 277515\n",
      "162 0.9616165161132812 266863 277515\n",
      "163 0.961692214012146 266884 277515\n",
      "164 0.9617318511009216 266895 277515\n",
      "165 0.9617750644683838 266907 277515\n",
      "166 0.9618111252784729 266917 277515\n",
      "167 0.9618543386459351 266929 277515\n",
      "168 0.9619371891021729 266952 277515\n",
      "169 0.9619948863983154 266968 277515\n",
      "170 0.9620633125305176 266987 277515\n",
      "171 0.9621390104293823 267008 277515\n",
      "172 0.9621678590774536 267016 277515\n",
      "173 0.9622002840042114 267025 277515\n",
      "174 0.9623479843139648 267066 277515\n",
      "175 0.9623876214027405 267077 277515\n",
      "176 0.9624885320663452 267105 277515\n",
      "177 0.9625678062438965 267127 277515\n",
      "178 0.9625930190086365 267134 277515\n",
      "179 0.9626326560974121 267145 277515\n",
      "180 0.9626975059509277 267163 277515\n",
      "181 0.9627191424369812 267169 277515\n",
      "182 0.9627587795257568 267180 277515\n",
      "183 0.9627839922904968 267187 277515\n",
      "184 0.9628020524978638 267192 277515\n",
      "185 0.9628308415412903 267200 277515\n",
      "186 0.9628632664680481 267209 277515\n",
      "187 0.9628885388374329 267216 277515\n",
      "188 0.9629497528076172 267233 277515\n",
      "189 0.9630398750305176 267258 277515\n",
      "190 0.9631155133247375 267279 277515\n",
      "191 0.9631443619728088 267287 277515\n",
      "192 0.9632344245910645 267312 277515\n",
      "193 0.9633101224899292 267333 277515\n",
      "194 0.963342547416687 267342 277515\n",
      "195 0.9633533358573914 267345 277515\n",
      "196 0.9634146094322205 267362 277515\n",
      "197 0.9634398221969604 267369 277515\n",
      "198 0.9644415974617004 267647 277515\n",
      "199 0.9644595980644226 267652 277515\n",
      "200 0.9644775986671448 267657 277515\n",
      "201 0.9644884467124939 267660 277515\n",
      "202 0.9645208716392517 267669 277515\n",
      "203 0.964675784111023 267712 277515\n",
      "204 0.9647766947746277 267740 277515\n",
      "205 0.964881181716919 267769 277515\n",
      "206 0.964881181716919 267769 277515\n",
      "207 0.9649064540863037 267776 277515\n",
      "208 0.9649280309677124 267782 277515\n",
      "209 0.9649388790130615 267785 277515\n",
      "210 0.9649568796157837 267790 277515\n",
      "211 0.9652703404426575 267877 277515\n",
      "212 0.9652956128120422 267884 277515\n",
      "213 0.9653388261795044 267896 277515\n",
      "214 0.9653964638710022 267912 277515\n",
      "215 0.9654289484024048 267921 277515\n",
      "216 0.9655262231826782 267948 277515\n",
      "217 0.9655478596687317 267954 277515\n",
      "218 0.9655550122261047 267956 277515\n",
      "219 0.9656127095222473 267972 277515\n",
      "220 0.9656451344490051 267981 277515\n",
      "221 0.965652346611023 267983 277515\n",
      "222 0.9657316207885742 268005 277515\n",
      "223 0.9657496213912964 268010 277515\n",
      "224 0.9658000469207764 268024 277515\n",
      "225 0.965814471244812 268028 277515\n",
      "226 0.9658324718475342 268033 277515\n",
      "227 0.9658505320549011 268038 277515\n",
      "228 0.9658901691436768 268049 277515\n",
      "229 0.9659658074378967 268070 277515\n",
      "230 0.9660739302635193 268100 277515\n",
      "231 0.9662793278694153 268157 277515\n",
      "232 0.9663369655609131 268173 277515\n",
      "233 0.9664198756217957 268196 277515\n",
      "234 0.9664234519004822 268197 277515\n",
      "235 0.9664595127105713 268207 277515\n",
      "236 0.9664703011512756 268210 277515\n",
      "237 0.9664955139160156 268217 277515\n",
      "238 0.9665351510047913 268228 277515\n",
      "239 0.9665675759315491 268237 277515\n",
      "240 0.9666144251823425 268250 277515\n",
      "241 0.9666504859924316 268260 277515\n",
      "242 0.9666649103164673 268264 277515\n",
      "243 0.9666901230812073 268271 277515\n",
      "244 0.9667441844940186 268286 277515\n",
      "245 0.9668306708335876 268310 277515\n",
      "246 0.9668486714363098 268315 277515\n",
      "247 0.9668774604797363 268323 277515\n",
      "248 0.9669207334518433 268335 277515\n",
      "249 0.9669927954673767 268355 277515\n",
      "250 0.9670072197914124 268359 277515\n",
      "251 0.9670648574829102 268375 277515\n",
      "252 0.9671081304550171 268387 277515\n",
      "253 0.9673999547958374 268468 277515\n",
      "254 0.9674108028411865 268471 277515\n",
      "255 0.9674180150032043 268473 277515\n",
      "256 0.9674648642539978 268486 277515\n",
      "257 0.9674720764160156 268488 277515\n",
      "258 0.96748286485672 268491 277515\n",
      "259 0.9677062630653381 268553 277515\n",
      "260 0.9677278995513916 268559 277515\n",
      "261 0.9678612351417542 268596 277515\n",
      "262 0.9678720235824585 268599 277515\n",
      "263 0.9679080843925476 268609 277515\n",
      "264 0.9679729342460632 268627 277515\n",
      "265 0.9679729342460632 268627 277515\n",
      "266 0.9679981470108032 268634 277515\n",
      "267 0.9680233597755432 268641 277515\n",
      "268 0.9681098461151123 268665 277515\n",
      "269 0.9681098461151123 268665 277515\n",
      "270 0.9681206941604614 268668 277515\n",
      "271 0.9681314826011658 268671 277515\n",
      "272 0.9681639075279236 268680 277515\n",
      "273 0.9682143330574036 268694 277515\n",
      "274 0.9682179689407349 268695 277515\n",
      "275 0.968235969543457 268700 277515\n",
      "276 0.9685350656509399 268783 277515\n",
      "277 0.9685639142990112 268791 277515\n",
      "278 0.9685782790184021 268795 277515\n",
      "279 0.9688629508018494 268874 277515\n",
      "280 0.9689494371414185 268898 277515\n",
      "281 0.9690070748329163 268914 277515\n",
      "282 0.9690107107162476 268915 277515\n",
      "283 0.9690431356430054 268924 277515\n",
      "284 0.9690647721290588 268930 277515\n",
      "285 0.969082772731781 268935 277515\n",
      "286 0.9694755673408508 269044 277515\n",
      "287 0.9694755673408508 269044 277515\n",
      "288 0.969493567943573 269049 277515\n",
      "289 0.9695043563842773 269052 277515\n",
      "290 0.9695872664451599 269075 277515\n",
      "291 0.9695944786071777 269077 277515\n",
      "292 0.9696376919746399 269089 277515\n",
      "293 0.9696593284606934 269095 277515\n",
      "294 0.969879150390625 269156 277515\n",
      "295 0.9698827266693115 269157 277515\n",
      "296 0.9700304865837097 269198 277515\n",
      "297 0.9700412750244141 269201 277515\n",
      "298 0.9700773358345032 269211 277515\n",
      "299 0.9701133370399475 269221 277515\n",
      "300 0.9702178239822388 269250 277515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 0.9702214598655701 269251 277515\n",
      "302 0.9702322483062744 269254 277515\n",
      "303 0.9702683091163635 269264 277515\n",
      "304 0.9702683091163635 269264 277515\n",
      "305 0.9702863097190857 269269 277515\n",
      "306 0.9703043103218079 269274 277515\n",
      "307 0.970315158367157 269277 277515\n",
      "308 0.9703367352485657 269283 277515\n",
      "309 0.9703475832939148 269286 277515\n",
      "310 0.9703511595726013 269287 277515\n",
      "311 0.9704052209854126 269302 277515\n",
      "312 0.9704124331474304 269304 277515\n",
      "313 0.9704196453094482 269306 277515\n",
      "314 0.9704304337501526 269309 277515\n",
      "315 0.9704340696334839 269310 277515\n",
      "316 0.9704412817955017 269312 277515\n",
      "317 0.970452070236206 269315 277515\n",
      "318 0.9704664945602417 269319 277515\n",
      "319 0.9704809188842773 269323 277515\n",
      "320 0.9704809188842773 269323 277515\n",
      "321 0.9704917073249817 269326 277515\n",
      "322 0.9705133438110352 269332 277515\n",
      "323 0.9705241322517395 269335 277515\n",
      "324 0.9705674052238464 269347 277515\n",
      "325 0.9705781936645508 269350 277515\n",
      "326 0.9705817699432373 269351 277515\n",
      "327 0.9705854058265686 269352 277515\n",
      "328 0.970596194267273 269355 277515\n",
      "329 0.9706178307533264 269361 277515\n",
      "330 0.9706178307533264 269361 277515\n",
      "331 0.9706322550773621 269365 277515\n",
      "332 0.9706466794013977 269369 277515\n",
      "333 0.9706718921661377 269376 277515\n",
      "334 0.9706718921661377 269376 277515\n",
      "335 0.970725953578949 269391 277515\n",
      "336 0.9707439541816711 269396 277515\n",
      "337 0.9707655906677246 269402 277515\n",
      "338 0.9707655906677246 269402 277515\n",
      "339 0.9708808660507202 269434 277515\n",
      "340 0.9709060788154602 269441 277515\n",
      "341 0.9709349274635315 269449 277515\n",
      "342 0.9709349274635315 269449 277515\n",
      "343 0.9709673523902893 269458 277515\n",
      "344 0.9709889888763428 269464 277515\n",
      "345 0.9710502624511719 269481 277515\n",
      "346 0.9710646271705627 269485 277515\n",
      "347 0.9710718393325806 269487 277515\n",
      "348 0.9710790514945984 269489 277515\n",
      "349 0.9711151123046875 269499 277515\n",
      "350 0.9711943864822388 269521 277515\n",
      "351 0.9712051749229431 269524 277515\n",
      "352 0.9712088108062744 269525 277515\n",
      "353 0.9712375998497009 269533 277515\n",
      "354 0.9712592363357544 269539 277515\n",
      "355 0.9712880849838257 269547 277515\n",
      "356 0.9712916612625122 269548 277515\n",
      "357 0.9713060855865479 269552 277515\n",
      "358 0.9713096618652344 269553 277515\n",
      "359 0.9713132977485657 269554 277515\n",
      "360 0.9713349342346191 269560 277515\n",
      "361 0.9713457226753235 269563 277515\n",
      "362 0.9713457226753235 269563 277515\n",
      "363 0.9713709354400635 269570 277515\n",
      "364 0.9714394211769104 269589 277515\n",
      "365 0.9714394211769104 269589 277515\n",
      "366 0.9714502096176147 269592 277515\n",
      "367 0.971479058265686 269600 277515\n",
      "368 0.971479058265686 269600 277515\n",
      "369 0.971504271030426 269607 277515\n",
      "370 0.971504271030426 269607 277515\n",
      "371 0.9715439081192017 269618 277515\n",
      "372 0.9715474843978882 269619 277515\n",
      "373 0.9715583324432373 269622 277515\n",
      "374 0.9729816913604736 270017 277515\n",
      "375 0.9729852676391602 270018 277515\n",
      "376 0.972992479801178 270020 277515\n",
      "377 0.9730032682418823 270023 277515\n",
      "378 0.9730069041252136 270024 277515\n",
      "379 0.9730393290519714 270033 277515\n",
      "380 0.9730465412139893 270035 277515\n",
      "381 0.9730573296546936 270038 277515\n",
      "382 0.9730609655380249 270039 277515\n",
      "383 0.9730609655380249 270039 277515\n",
      "384 0.9730609655380249 270039 277515\n",
      "385 0.9730609655380249 270039 277515\n",
      "386 0.9730609655380249 270039 277515\n",
      "387 0.9730645418167114 270040 277515\n",
      "388 0.9730645418167114 270040 277515\n",
      "389 0.9730789661407471 270044 277515\n",
      "390 0.9731330275535583 270059 277515\n",
      "391 0.9731510281562805 270064 277515\n",
      "392 0.9731762409210205 270071 277515\n",
      "393 0.9731870293617249 270074 277515\n",
      "394 0.9732158780097961 270082 277515\n",
      "395 0.9732303023338318 270086 277515\n",
      "396 0.9732447266578674 270090 277515\n",
      "397 0.9732591509819031 270094 277515\n",
      "398 0.9733384251594543 270116 277515\n",
      "399 0.9733960628509521 270132 277515\n",
      "400 0.97340327501297 270134 277515\n",
      "401 0.9734140634536743 270137 277515\n",
      "402 0.9734212756156921 270139 277515\n",
      "403 0.97342848777771 270141 277515\n",
      "404 0.9734320640563965 270142 277515\n",
      "405 0.9734356999397278 270143 277515\n",
      "406 0.9734645485877991 270151 277515\n",
      "407 0.9734681248664856 270152 277515\n",
      "408 0.9734681248664856 270152 277515\n",
      "409 0.9734753370285034 270154 277515\n",
      "410 0.9734789133071899 270155 277515\n",
      "411 0.9734789133071899 270155 277515\n",
      "412 0.9734969735145569 270160 277515\n",
      "413 0.9735041856765747 270162 277515\n",
      "414 0.9736374616622925 270199 277515\n",
      "415 0.9736483097076416 270202 277515\n",
      "416 0.9736555218696594 270204 277515\n",
      "417 0.9739113450050354 270275 277515\n",
      "418 0.973925769329071 270279 277515\n",
      "419 0.973925769329071 270279 277515\n",
      "420 0.9739293456077576 270280 277515\n",
      "421 0.9739545583724976 270287 277515\n",
      "422 0.9739581942558289 270288 277515\n",
      "423 0.9739906191825867 270297 277515\n",
      "424 0.9739941954612732 270298 277515\n",
      "425 0.9740086197853088 270302 277515\n",
      "426 0.9740158319473267 270304 277515\n",
      "427 0.9740266799926758 270307 277515\n",
      "428 0.9740302562713623 270308 277515\n",
      "429 0.9740302562713623 270308 277515\n",
      "430 0.9740410447120667 270311 277515\n",
      "431 0.9740482568740845 270313 277515\n",
      "432 0.9740518927574158 270314 277515\n",
      "433 0.9740626811981201 270317 277515\n",
      "434 0.9740734696388245 270320 277515\n",
      "435 0.9740806818008423 270322 277515\n",
      "436 0.9740915298461914 270325 277515\n",
      "437 0.9740987420082092 270327 277515\n",
      "438 0.974105954170227 270329 277515\n",
      "439 0.9742140173912048 270359 277515\n",
      "440 0.974224865436554 270362 277515\n",
      "441 0.974224865436554 270362 277515\n",
      "442 0.9742284417152405 270363 277515\n",
      "443 0.9742392301559448 270366 277515\n",
      "444 0.9742428660392761 270367 277515\n",
      "445 0.974250078201294 270369 277515\n",
      "446 0.9742536544799805 270370 277515\n",
      "447 0.9742536544799805 270370 277515\n",
      "448 0.9742536544799805 270370 277515\n",
      "449 0.9742536544799805 270370 277515\n",
      "450 0.9743473529815674 270396 277515\n",
      "451 0.9743545651435852 270398 277515\n",
      "452 0.9743545651435852 270398 277515\n",
      "453 0.9743581414222717 270399 277515\n",
      "454 0.9743581414222717 270399 277515\n",
      "455 0.9744230508804321 270417 277515\n",
      "456 0.9744230508804321 270417 277515\n",
      "457 0.974437415599823 270421 277515\n",
      "458 0.9745095372200012 270441 277515\n",
      "459 0.9745095372200012 270441 277515\n",
      "460 0.9745635390281677 270456 277515\n",
      "461 0.9745707511901855 270458 277515\n",
      "462 0.9745851755142212 270462 277515\n",
      "463 0.9745851755142212 270462 277515\n",
      "464 0.9745851755142212 270462 277515\n",
      "465 0.9745888113975525 270463 277515\n",
      "466 0.9745995998382568 270466 277515\n",
      "467 0.974617600440979 270471 277515\n",
      "468 0.9746248126029968 270473 277515\n",
      "469 0.9746356010437012 270476 277515\n",
      "470 0.974642813205719 270478 277515\n",
      "471 0.9746644496917725 270484 277515\n",
      "472 0.9746788740158081 270488 277515\n",
      "473 0.9746788740158081 270488 277515\n",
      "474 0.9747509360313416 270508 277515\n",
      "475 0.9747617244720459 270511 277515\n",
      "476 0.9747689366340637 270513 277515\n",
      "477 0.9747761487960815 270515 277515\n",
      "478 0.9747797846794128 270516 277515\n",
      "479 0.9747833609580994 270517 277515\n",
      "480 0.9747833609580994 270517 277515\n",
      "481 0.9747905731201172 270519 277515\n",
      "482 0.9747905731201172 270519 277515\n",
      "483 0.9747942090034485 270520 277515\n",
      "484 0.9748013615608215 270522 277515\n",
      "485 0.9748049974441528 270523 277515\n",
      "486 0.9748085737228394 270524 277515\n",
      "487 0.9748554229736328 270537 277515\n",
      "488 0.9748626351356506 270539 277515\n",
      "489 0.9748626351356506 270539 277515\n",
      "490 0.9748842716217041 270545 277515\n",
      "491 0.9749166965484619 270554 277515\n",
      "492 0.9749166965484619 270554 277515\n",
      "493 0.9749166965484619 270554 277515\n",
      "494 0.9749166965484619 270554 277515\n",
      "495 0.9749166965484619 270554 277515\n",
      "496 0.9749563336372375 270565 277515\n",
      "497 0.9749563336372375 270565 277515\n",
      "498 0.9749599099159241 270566 277515\n",
      "499 0.9749599099159241 270566 277515\n",
      "500 0.9749635457992554 270567 277515\n",
      "501 0.9749671220779419 270568 277515\n",
      "502 0.9749671220779419 270568 277515\n",
      "503 0.9749707579612732 270569 277515\n",
      "504 0.9749743342399597 270570 277515\n",
      "505 0.9749851822853088 270573 277515\n",
      "506 0.9749995470046997 270577 277515\n",
      "507 0.9750103950500488 270580 277515\n",
      "508 0.9750103950500488 270580 277515\n",
      "509 0.9750103950500488 270580 277515\n",
      "510 0.9750139713287354 270581 277515\n",
      "511 0.9750248193740845 270584 277515\n"
     ]
    }
   ],
   "source": [
    "na=NeuronsActivate(model,data,0.0)\n",
    "neurons_activate=na.get_neurons_activate()\n",
    "covered_neurons=0\n",
    "total_neurons=neurons_activate.shape[1]\n",
    "covered_dict=torch.zeros([total_neurons]).to(device)\n",
    "\n",
    "for sample_index, sample_activation in enumerate(neurons_activate):\n",
    "    #print(sample_index)\n",
    "    covered_dict+=sample_activation\n",
    "    covered_dict=torch.gt(covered_dict,0)\n",
    "    covered_neurons=torch.sum(covered_dict)\n",
    "    covered_dict=covered_dict.float()\n",
    "    coverage= covered_neurons / float(total_neurons)\n",
    "    print(sample_index,coverage.item(),covered_neurons.item(),total_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb0bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照样本的神经元覆盖率进行排序\n",
    "class NC():\n",
    "    def __init__(self,model,test,t):#model模型，test测试用例集，t神经元激活门限\n",
    "        self.test = test\n",
    "        self.t = t\n",
    "        self.lst = []\n",
    "        index_lst = []\n",
    "        self.lst = list(zip(index_lst, self.lst))\n",
    "        \n",
    "        self.neuron_activate = []\n",
    "        self.neuron_num = 0\n",
    "\n",
    "    def fit(self):\n",
    "        batch_size=128\n",
    "        datalist=torch.split(data, batch_size, dim=0)\n",
    "        neurons_activate=[]\n",
    "        for data_batch in datalist:         \n",
    "            na=NeuronsActivate(model,data_batch,self.t)\n",
    "            batch_neurons_activate=na.get_neurons_activate()\n",
    "            upper = (batch_neurons_activate > self.t)  # nc准则：大于门限就是激活\n",
    "            batch_coverage = np.sum(upper,axis=1)  # 统计激活了的神经元的个数\n",
    "            neurons_activate.append(batch_coverage)           \n",
    "            \n",
    "        self.neuron_activate=np.concatenate(neurons_activate, axis=0)\n",
    "\n",
    "    def rank_fast(self):\n",
    "        rank_lst = np.argsort(self.neuron_activate)  # 按照值从小到大排序,因此序号越小代表值越小代表越好\n",
    "        return rank_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d071c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliang/anaconda3/envs/logic/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "# 可以贪心排\n",
    "# 可以单个样本比较排\n",
    "# 0 0.5 1\n",
    "\n",
    "ac = NC(model,data.to(device),0.75)\n",
    "rate = ac.fit()\n",
    "start = time.time()\n",
    "rank_lst = ac.rank_fast()\n",
    "end = time.time()\n",
    "rank_lst_time = start - end\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "pred_test = model(data.to(device)).argmax(dim=1).cpu().numpy()\n",
    "true_test=true_test.cpu().numpy()\n",
    "df['right'] = (pred_test == true_test).astype('int')\n",
    "df['cam'] = 0\n",
    "df['cam'].loc[rank_lst] = list(range(1, len(rank_lst) + 1))\n",
    "df['cam_time'] = rank_lst_time\n",
    "df['rate'] = rate\n",
    "df['ctm'] = 0\n",
    "df['ctm'].loc[rank_lst] = list(range(1, len(rank_lst) + 1))\n",
    "df['ctm_time'] = rank_lst_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f96e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./all_output/output_cifar/{}/{}_nac_t_{}.csv'.format('vgg16', 'cifar', 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd4f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
