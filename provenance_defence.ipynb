{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Robustness Toolbox for Provenance-Based Defenses\n",
    "\n",
    "In this notebook we will learn how to use ART to defend against adversarial attacks in IoT settings.\n",
    "\n",
    "When data is collected from multiple sources, we can use **provenance features** to track the origin of that data. Using those features, we can defend models against malicious attacks. We will also show how to use the Reject on Negative Impact (RONI) defense method within ART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/miniconda3/envs/art/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys\n",
    "from os.path import abspath\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from art.attacks.poisoning.poisoning_attack_svm import PoisoningAttackSVM\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnSVC\n",
    "from art.defences.detector.poison import ProvenanceDefense, RONIDefense\n",
    "from art.utils import load_mnist\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(301)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 40\n",
    "num_poison = 5\n",
    "num_valid = 40 # the number of valid examples for the attacker\n",
    "num_trusted = 25 # the number of trusted data for the defender\n",
    "num_devices = 4 # last device is inserting poison\n",
    "kernel = 'linear' # available kernels are 'rbf', 'poly' and 'linear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform MNIST data\n",
    "\n",
    "In this examples we are training a classifier that differentiates between the number 4 and the number 0. The training data is split between the first `num_devices - 1` devices and the poisoned training data is the added to the last device. Quantity fo data and model kernel are specified by hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_mnist()\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "zero_or_four = np.logical_or(y_train == 4, y_train == 0)\n",
    "x_train = x_train[zero_or_four]\n",
    "y_train = y_train[zero_or_four]\n",
    "tr_labels = np.zeros((y_train.shape[0], 2))\n",
    "tr_labels[y_train == 0] = np.array([1, 0])\n",
    "tr_labels[y_train == 4] = np.array([0, 1])\n",
    "y_train = tr_labels\n",
    "\n",
    "\n",
    "zero_or_four = np.logical_or(y_test == 4, y_test == 0)\n",
    "x_test = x_test[zero_or_four]\n",
    "y_test = y_test[zero_or_four]\n",
    "te_labels = np.zeros((y_test.shape[0], 2))\n",
    "te_labels[y_test == 0] = np.array([1, 0])\n",
    "te_labels[y_test == 4] = np.array([0, 1])\n",
    "y_test = te_labels\n",
    "\n",
    "n_samples_train = x_train.shape[0]\n",
    "n_features_train = x_train.shape[1] * x_train.shape[2] * x_train.shape[3]\n",
    "n_samples_test = x_test.shape[0]\n",
    "n_features_test = x_test.shape[1] * x_test.shape[2] * x_test.shape[3]\n",
    "\n",
    "x_train = x_train.reshape(n_samples_train, n_features_train)\n",
    "x_test = x_test.reshape(n_samples_test, n_features_test)\n",
    "x_train = x_train[:num_training]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "trusted_data = x_test[:num_trusted]\n",
    "trusted_labels = y_test[:num_trusted]\n",
    "x_test = x_test[num_trusted:]\n",
    "y_test = y_test[num_trusted:]\n",
    "valid_data = x_test[:num_valid]\n",
    "valid_labels = y_test[:num_valid]\n",
    "x_test = x_test[num_valid:]\n",
    "y_test = y_test[num_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add provenance data and poison samples\n",
    "\n",
    "*Note:* In real application scenarios, provenance data is also loaded. Provenance data is generated for this experiment for demonstration purposes.\n",
    "\n",
    "This code will take longer to run depending on the number of poison samples you allow. Each samples is being generated independently, iteratively maximizing the generalization loss of the original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVM poisoning: 5it [00:24,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# assign random provenance features to the original training points\n",
    "clean_prov = np.random.randint(num_devices - 1, size=x_train.shape[0])\n",
    "p_train = np.eye(num_devices)[clean_prov]\n",
    "\n",
    "no_defense = ScikitlearnSVC(model=SVC(kernel=kernel), clip_values=(min_, max_))\n",
    "no_defense.fit(x=x_train, y=y_train)\n",
    "# poison a predetermined number of points starting at training points\n",
    "poison_points = np.random.randint(no_defense._model.support_vectors_.shape[0], size=num_poison)\n",
    "all_poison_init = np.copy(no_defense._model.support_vectors_[poison_points])\n",
    "poison_labels = np.array([1,1]) - no_defense.predict(all_poison_init)\n",
    "\n",
    "\n",
    "svm_attack = PoisoningAttackSVM(classifier=no_defense, x_train=x_train, y_train=y_train,\n",
    "                                step=0.1, eps=1.0, x_val=valid_data, y_val=valid_labels, max_iter=200)\n",
    "\n",
    "poisoned_data, _ = svm_attack.poison(all_poison_init, y=poison_labels)\n",
    "\n",
    "# Stack on poison to data and add provenance of bad actor\n",
    "all_data = np.vstack([x_train, poisoned_data])\n",
    "all_labels = np.vstack([y_train, poison_labels])\n",
    "poison_prov = np.zeros((num_poison, num_devices))\n",
    "poison_prov[:,num_devices - 1] = 1\n",
    "all_p = np.vstack([p_train, poison_prov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train clean classifier and poisoned classifier\n",
    "perfect_defense = ScikitlearnSVC(model=SVC(kernel=kernel), clip_values=(min_, max_))\n",
    "perfect_defense.fit(x=x_train, y=y_train)\n",
    "no_defense.fit(x=all_data, y=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect defense accuracy (trusted set) 97.68%\n",
      "No defense accuracy (trusted set) 77.91%\n"
     ]
    }
   ],
   "source": [
    "perf_acc = np.average(np.all(perfect_defense.predict(x_test) == y_test, axis=1)) * 100\n",
    "no_acc = np.average(np.all(no_defense.predict(x_test) == y_test, axis=1)) * 100\n",
    "print(\"Perfect defense accuracy (trusted set) {0:.2f}%\".format(perf_acc))\n",
    "print(\"No defense accuracy (trusted set) {0:.2f}%\".format(no_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Defenses\n",
    "\n",
    "We will apply the following defenses to this poisoning attack:\n",
    "* **Perfect Defense** — All poison is detected and model is trained on clean data.\n",
    "* **Provenance-Based Defense with Trusted Data** — Poison is detected using the provenance defense algorithm specified above.\n",
    "* **Provenance-Based Defense without Trusted Data** — Assuming no validation data, just check each data segment for suspected poison.\n",
    "* **No defense** — Model is trained with poisoned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provenance Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provenenace defense method checks the effect of removing segments of the data that may come a bad actor intentionally poisoning the data. When a sector is found that is potentially poisonous, it is flagged as suspicious.\n",
    "\n",
    "In the trusted data version of the algorithm, the defender has some handpicked trusted data to test the performance of the model. In the version of the algorithm without trusted data, a random subset of training points from all segments are used as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "prov_defense_trust = ProvenanceDefense(no_defense, all_data, all_labels, all_p, \n",
    "                                       x_val=trusted_data, y_val=trusted_labels, eps=0.1)\n",
    "prov_defense_trust.detect_poison()\n",
    "prov_defense_no_trust = ProvenanceDefense(no_defense, all_data, all_labels, all_p, eps=0.1)\n",
    "prov_defense_no_trust.detect_poison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Defenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_is_clean = np.array([1 if i < num_training else 0 for i in range(len(all_data))])\n",
    "def evaluate_defense(defense, name):\n",
    "    print(\"\\nEvaluating results of {} defense...\".format(name))\n",
    "    pc_tp = np.average(real_is_clean[:num_training] == defense.is_clean_lst[:num_training]) * 100\n",
    "    pc_tn = np.average(real_is_clean[num_training:] == defense.is_clean_lst[num_training:]) * 100\n",
    "    print(\"Percent of normal points correctly labeled (True Negative): {0:.2f}%\".format(pc_tp))\n",
    "    print(\"Percent of poison points correctly labeled (True Positive): {0:.2f}%\".format(pc_tn))\n",
    "    \n",
    "    classifier = ScikitlearnSVC(model=SVC(kernel=kernel), clip_values=(min_, max_))\n",
    "    mask = np.array(defense.is_clean_lst) == 1\n",
    "    classifier.fit(all_data[mask], all_labels[mask])\n",
    "    acc = np.average(np.all(classifier.predict(x_test) == y_test, axis=1)) * 100\n",
    "    print(\"Accuracy of classifier trained with {0:.2f} filter on test set\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating results of Provenance Defense w/o Trusted Data defense...\n",
      "Percent of normal points correctly labeled (True Negative): 100.00%\n",
      "Percent of poison points correctly labeled (True Positive): 0.00%\n",
      "Accuracy of classifier trained with 77.91 filter on test set\n",
      "\n",
      "Evaluating results of Provenance Defense w/ Trusted Data defense...\n",
      "Percent of normal points correctly labeled (True Negative): 70.00%\n",
      "Percent of poison points correctly labeled (True Positive): 100.00%\n",
      "Accuracy of classifier trained with 97.84 filter on test set\n"
     ]
    }
   ],
   "source": [
    "evaluate_defense(prov_defense_no_trust, \"Provenance Defense w/o Trusted Data\")\n",
    "evaluate_defense(prov_defense_trust, \"Provenance Defense w/ Trusted Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba68032bb30de2c07063716433fb3bfc5ec23705c6d6e3e39c5c9be87fecbadd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
